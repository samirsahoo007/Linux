http://michal.karzynski.pl/blog/2014/05/18/setting-up-an-asynchronous-task-queue-for-django-using-celery-redis/
Setting up an asynchronous task queue for Django using Celery and Redis
Celery is a powerful, production-ready asynchronous job queue, which allows you to run time-consuming Python functions in the background. A Celery powered application can respond to user requests quickly, while long-running tasks are passed onto the queue. In this article we will demonstrate how to add Celery to a Django application using Redis.
Celery uses a broker to pass messages between your application and Celery worker processes. In this article we will set up Redis as the message broker. You should note that persistence is not the main goal of this data store, so your queue could be erased in the event of a power failure or other crash. Keep this in mind and don’t use the job queue to store application state. If you need your queue to be have persistence, use another message broker such as RabbitMQ.

Synchronous execution means the execution happens in a single series. A->B->C->D. If you are calling those routines, A will run, then finish, then B will start, then finish, then C will start, etc.
With Asynchronous execution, you begin a routine, and let it run in the background while you start your next, then at some point, say "wait for this to finish". It's more like:
Start A->B->C->D->Wait for A to finish
The advantage is that you can execute B, C, and or D while A is still running (in the background, on a separate thread), so you can take better advantage of your resources and have fewer "hangs" or "waits".

In simpler terms:
SYNCHRONOUS
You are in a queue to get a movie ticket. You cannot get one until everybody in front of you gets one, and the same applies to the people queued behind you.
ASYNCHRONOUS
You are in a restaurant with many other people. You order your food. Other people can also order their food, they don't have to wait for your food to be cooked and served to you before they can order. In the kitchen restaurant workers are continuously cooking, serving, and taking orders. People will get their food served as soon as it is cooked.
This document describes the current stable version of Celery (4.1). For development docs, go here. 
http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html
First Steps with Celery¶
Celery is a task queue with batteries included. It’s easy to use. It’s designed around best practices so that your product can scale and integrate with other languages, and it comes with the tools and support you need to run such a system in production.
Learn about;
•	Choosing and installing a message transport (broker).
•	Installing Celery and creating your first task.
•	Starting the worker and calling tasks.
•	Keeping track of tasks as they transition through different states, and inspecting return values.
Celery may seem daunting at first - but don’t worry - this tutorial will get you started in no time. It’s deliberately kept simple, so as to not confuse you with advanced features. After you have finished this tutorial, it’s a good idea to browse the rest of the documentation. For example the Next Steps tutorial will showcase Celery’s capabilities.
•	Choosing a Broker
o	RabbitMQ
o	Redis
o	Other brokers
•	Installing Celery
•	Application
•	Running the Celery worker server
•	Calling the task
•	Keeping Results
•	Configuration
•	Where to go from here
•	Troubleshooting
o	Worker doesn’t start: Permission Error
o	Result backend doesn’t work or tasks are always in PENDING state
Choosing a Broker¶
Celery requires a solution to send and receive messages; usually this comes in the form of a separate service called a message broker.
There are several choices available, including:
RabbitMQ¶
RabbitMQ is feature-complete, stable, durable and easy to install. It’s an excellent choice for a production environment. Detailed information about using RabbitMQ with Celery:
Using RabbitMQ
If you’re using Ubuntu or Debian install RabbitMQ by executing this command:
$ sudo apt-get install rabbitmq-server
When the command completes, the broker will already be running in the background, ready to move messages for you: Starting rabbitmq-server: SUCCESS.
Don’t worry if you’re not running Ubuntu or Debian, you can go to this website to find similarly simple installation instructions for other platforms, including Microsoft Windows:
http://www.rabbitmq.com/download.html
Redis¶
Redis is also feature-complete, but is more susceptible to data loss in the event of abrupt termination or power failures. Detailed information about using Redis:
Using Redis
Other brokers¶
In addition to the above, there are other experimental transport implementations to choose from, including Amazon SQS.
See Broker Overview for a full list.
Installing Celery¶
Celery is on the Python Package Index (PyPI), so it can be installed with standard Python tools like pip or easy_install:
$ pip install celery
Application
The first thing you need is a Celery instance. As this instance is used as the entry-point for everything you want to do in Celery, like creating tasks and managing workers, it must be possible for other modules to import it.
In this tutorial we keep everything contained in a single module, but for larger projects you want to create a dedicated module.
Let’s create the file tasks.py:
from celery import Celery

app = Celery('tasks', broker='pyamqp://guest@localhost//')

@app.task
def add(x, y):
    return x + y
The first argument to Celery is the name of the current module. This is only needed so that names can be automatically generated when the tasks are defined in the __main__ module.
The second argument is the broker keyword argument, specifying the URL of the message broker you want to use. Here using RabbitMQ (also the default option).
See Choosing a Broker above for more choices – for RabbitMQ you can use amqp://localhost, or for Redis you can use redis://localhost.
You defined a single task, called add, returning the sum of two numbers.
Running the Celery worker server¶
You can now run the worker by executing our program with the worker argument:
$ celery -A tasks worker --loglevel=info
Note
See the Troubleshooting section if the worker doesn’t start.
In production you’ll want to run the worker in the background as a daemon. To do this you need to use the tools provided by your platform, or something like supervisord (see Daemonization for more information).
For a complete listing of the command-line options available, do:
$  celery worker --help
There are also several other commands available, and help is also available:
$ celery help
Calling the task
To call our task you can use the delay() method.
This is a handy shortcut to the apply_async() method that gives greater control of the task execution (see Calling Tasks):
>>> from tasks import add
>>> add.delay(4, 4)
The task has now been processed by the worker you started earlier. You can verify this by looking at the worker’s console output.
Calling a task returns an AsyncResult instance. This can be used to check the state of the task, wait for the task to finish, or get its return value (or if the task failed, to get the exception and traceback).
Results are not enabled by default. In order to do remote procedure calls or keep track of task results in a database, you will need to configure Celery to use a result backend. This is described in the next section.
Keeping Results
If you want to keep track of the tasks’ states, Celery needs to store or send the states somewhere. There are several built-in result backends to choose from: SQLAlchemy/Django ORM, Memcached, Redis, RPC (RabbitMQ/AMQP), and – or you can define your own.
For this example we use the rpc result backend, that sends states back as transient messages. The backend is specified via the backend argument to Celery, (or via the result_backend setting if you choose to use a configuration module):
app = Celery('tasks', backend='rpc://', broker='pyamqp://')
Or if you want to use Redis as the result backend, but still use RabbitMQ as the message broker (a popular combination):
app = Celery('tasks', backend='redis://localhost', broker='pyamqp://')
To read more about result backends please see Result Backends.
Now with the result backend configured, let’s call the task again. This time you’ll hold on to the AsyncResult instance returned when you call a task:
>>> result = add.delay(4, 4)
The ready() method returns whether the task has finished processing or not:
>>> result.ready()
False
You can wait for the result to complete, but this is rarely used since it turns the asynchronous call into a synchronous one:
>>> result.get(timeout=1)
8
In case the task raised an exception, get() will re-raise the exception, but you can override this by specifying the propagate argument:
>>> result.get(propagate=False)
If the task raised an exception, you can also gain access to the original traceback:
>>> result.traceback
?
See celery.result for the complete result object reference.
Configuration¶
Celery, like a consumer appliance, doesn’t need much configuration to operate. It has an input and an output. The input must be connected to a broker, and the output can be optionally connected to a result backend. However, if you look closely at the back, there’s a lid revealing loads of sliders, dials, and buttons: this is the configuration.
The default configuration should be good enough for most use cases, but there are many options that can be configured to make Celery work exactly as needed. Reading about the options available is a good idea to familiarize yourself with what can be configured. You can read about the options in the Configuration and defaults reference.
The configuration can be set on the app directly or by using a dedicated configuration module. As an example you can configure the default serializer used for serializing task payloads by changing the task_serializer setting:
app.conf.task_serializer = 'json'
If you’re configuring many settings at once you can use update:
app.conf.update(
    task_serializer='json',
    accept_content=['json'],  # Ignore other content
    result_serializer='json',
    timezone='Europe/Oslo',
    enable_utc=True,
)
For larger projects, a dedicated configuration module is recommended. Hard coding periodic task intervals and task routing options is discouraged. It is much better to keep these in a centralized location. This is especially true for libraries, as it enables users to control how their tasks behave. A centralized configuration will also allow your SysAdmin to make simple changes in the event of system trouble.
You can tell your Celery instance to use a configuration module by calling the app.config_from_object() method:
app.config_from_object('celeryconfig')
This module is often called “celeryconfig”, but you can use any module name.
In the above case, a module named celeryconfig.py must be available to load from the current directory or on the Python path. It could look something like this:
celeryconfig.py:
broker_url = 'pyamqp://'
result_backend = 'rpc://'

task_serializer = 'json'
result_serializer = 'json'
accept_content = ['json']
timezone = 'Europe/Oslo'
enable_utc = True
To verify that your configuration file works properly and doesn’t contain any syntax errors, you can try to import it:
$ python -m celeryconfig
For a complete reference of configuration options, see Configuration and defaults.
To demonstrate the power of configuration files, this is how you’d route a misbehaving task to a dedicated queue:
celeryconfig.py:
task_routes = {
    'tasks.add': 'low-priority',
}
Or instead of routing it you could rate limit the task instead, so that only 10 tasks of this type can be processed in a minute (10/m):
celeryconfig.py:
task_annotations = {
    'tasks.add': {'rate_limit': '10/m'}
}
If you’re using RabbitMQ or Redis as the broker then you can also direct the workers to set a new rate limit for the task at runtime:
$ celery -A tasks control rate_limit tasks.add 10/m
worker@example.com: OK
    new rate limit set successfully
See Routing Tasks to read more about task routing, and the task_annotations setting for more about annotations, or Monitoring and Management Guide for more about remote control commands and how to monitor what your workers are doing.
Where to go from here¶
If you want to learn more you should continue to the Next Steps tutorial, and after that you can read the User Guide.
Troubleshooting¶
There’s also a troubleshooting section in the Frequently Asked Questions.
Worker doesn’t start: Permission Error¶
•	If you’re using Debian, Ubuntu or other Debian-based distributions:
Debian recently renamed the /dev/shm special file to /run/shm.
A simple workaround is to create a symbolic link:
# ln -s /run/shm /dev/shm
•	Others:
If you provide any of the --pidfile, --logfile or --statedb arguments, then you must make sure that they point to a file or directory that’s writable and readable by the user starting the worker.
Result backend doesn’t work or tasks are always in PENDING state¶
All tasks are PENDING by default, so the state would’ve been better named “unknown”. Celery doesn’t update the state when a task is sent, and any task with no history is assumed to be pending (you know the task id, after all).
1.	Make sure that the task doesn’t have ignore_result enabled.
Enabling this option will force the worker to skip updating states.
2.	Make sure the task_ignore_result setting isn’t enabled.
3.	Make sure that you don’t have any old workers still running.
It’s easy to start multiple workers by accident, so make sure that the previous worker is properly shut down before you start a new one.
An old worker that isn’t configured with the expected result backend may be running and is hijacking the tasks.
The --pidfile argument can be set to an absolute path to make sure this doesn’t happen.
4.	Make sure the client is configured with the right backend.
If, for some reason, the client is configured to use a different backend than the worker, you won’t be able to receive the result. Make sure the backend is configured correctly:
>>> result = task.delay()
>>> print(result.backend)


Prerequisites
In this article we will add Celery to a Django application running in a Python virtualenv. I will assume that the virtual environment is located in the directory /webapps/hello_django/ and that the application is up an running. You can follow steps in my previous article to set up Django in virtualenv running on Nginx and Gunicorn.
This article was tested on a server running Debian 7.
Install Redis
$ sudo aptitude install redis-server
$ redis-server --version
Redis server version 2.4.14 (00000000:0)
Check if Redis is up and accepting connections:
$ redis-cli ping
PONG
Installing Celery in your aplication’s virtualenv
 (hello_django)hello@django:~$ pip install celery[redis]
Downloading/unpacking celery[redis]
(...)
Successfully installed celery pytz billiard kombu redis anyjson amqp
Cleaning up...
Setting up Celery support in your Django application
In order to use Celery as part of your Django application you’ll need to create a few files and tweak some settings. Let’s start by adding Celery-related configuration variables to settings.py
/webapps/hello_django/hello/hello/settings.py 
1
2
3
4
5
	# CELERY SETTINGS
BROKER_URL = 'redis://localhost:6379/0'
CELERY_ACCEPT_CONTENT = ['json']
CELERY_TASK_SERIALIZER = 'json'
CELERY_RESULT_SERIALIZER = 'json'

Now we’ll create a file named celery.py, which will instantiate Celery, creating a so called Celery application. You can find more information about available Celery application settings in the documentation.
/webapps/hello_django/hello/hello/celery.py 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
	from __future__ import absolute_import

import os
from celery import Celery
from django.conf import settings

# set the default Django settings module for the 'celery' program.
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'hello.settings')

app = Celery('hello_django')

# Using a string here means the worker will not have to
# pickle the object when using Windows.
app.config_from_object('django.conf:settings')
app.autodiscover_tasks(lambda: settings.INSTALLED_APPS)
In order to instantiate the Celery app every time our Django application is started, we can add the following lines to the __init__.py file in our Django proj.proj module. This will make sure that celery task use this app.
/webapps/hello_django/hello/hello/__init__.py 
1
2
	from __future__ import absolute_import
from .celery import app as celery_app

Create your first tasks
We will now add an app called testapp to our Django project and add some tasks to this app. Let’s start by creating the app:
(hello_django)hello@django:~/hello$ python manage.py startapp testapp
Make sure that the app is added to INSTALLED_APPS in settings.py
/webapps/hello_django/hello/hello/settings.py 
1
2
3
4
	INSTALLED_APPS = (
    # (...)
    'testapp',
)

Create a file called tasks.py in your apps’s directory and add the code of your first Celery task to the file.
/webapps/hello_django/hello/testapp/tasks.py 
1
2
3
4
5
6
7
	from __future__ import absolute_import

from celery import shared_task

@shared_task
def test(param):
    return 'The test task executed with argument "%s" ' % param

Find more information about writing task functions in the docs.
If you created all files as outlined above, you should see the following directory structure:
/webapps/hello_django/hello
├── hello
│   ├── celery.py       # The Celery app file
│   ├── __init__.py     # The project module file we modified
│   ├── settings.py     # Settings go here, obviously :)
│   ├── urls.py
│   └── wsgi.py
├── manage.py
└── testapp
    ├── __init__.py
    ├── models.py
    ├── tasks.py        # File containing tasks for this app
    ├── tests.py
    └── views.py
You can find a complete sample Django project on Celery’s GitHub.
Testing the setup
In production we will want Celery workers to be daemonized, but let’s just quickly start the workers to check that everything is configured correctly. Use the celery command located in your virtualenv’s bin directory to start the workers. Make sure that the module path hello.celery:app is available on your PYTHONPATH.
It’s important to understand how Celery names tasks which it discovers and how these names are related to Python module paths. If you run into NotRegistered or ImportError exceptions make sure that your apps and tasks are imported in a consistent manner and your PYTHONPATH is set correctly.
$ export PYTHONPATH=/webapps/hello_django/hello:$PYTHONPATH
$ /webapps/hello_django/bin/celery --app=hello.celery:app worker --loglevel=INFO

 -------------- celery@django v3.1.11 (Cipater)
---- **** -----
--- * ***  * -- Linux-3.2.0-4-amd64-x86_64-with-debian-7.5
-- * - **** ---
- ** ---------- [config]
- ** ---------- .> app:         hello_django:0x15ae410
- ** ---------- .> transport:   redis://localhost:6379/0
- ** ---------- .> results:     disabled
- *** --- * --- .> concurrency: 2 (prefork)
-- ******* ----
--- ***** ----- [queues]
 -------------- .> celery           exchange=celery(direct) key=celery

[tasks]
  . testapp.tasks.test

[2014-05-20 13:53:59,740: INFO/MainProcess] Connected to redis://localhost:6379/0
[2014-05-20 13:53:59,748: INFO/MainProcess] mingle: searching for neighbors
[2014-05-20 13:54:00,756: INFO/MainProcess] mingle: all alone
[2014-05-20 13:54:00,769: WARNING/MainProcess] celery@django ready.
If everything worked, you should see a splash screen similar to the above and the [tasks] section should list tasks discovered in all the apps of your project.
[tasks]
  . testapp.tasks.test
Submitting a task to the queue for execution
In another terminal, activate the virtualenv and start a task from your project’s shell.
$ sudo su - hello
hello@django:~$ source bin/activate
(hello_django)hello@django:~$ cd hello/
(hello_django)hello@django:~/hello$ python manage.py shell
Python 2.7.3 (default, Mar 13 2014, 11:03:55)
[GCC 4.7.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
(InteractiveConsole)
>>> from testapp.tasks import test
>>> test.delay('This is just a test!')
<AsyncResult: 79e35cf7-0a3d-4786-b746-2d3dd45a5c16>
You should see messages appear in the terminal where Celery workers are started:
[2014-05-18 11:43:24,801: INFO/MainProcess] Received task: testapp.tasks.test[79e35cf7-0a3d-4786-b746-2d3dd45a5c16]
[2014-05-18 11:43:24,804: INFO/MainProcess] Task testapp.tasks.test[79e35cf7-0a3d-4786-b746-2d3dd45a5c16] succeeded in 0.00183034200018s: u'The test task executed with argument "This is just a test!" '
You can find more information about calling Celery tasks in the docs.
Running Celery workers as daemons
In production we can use supervisord to start Celery workers and make sure they are restarted in case of a system reboot or crash. Installation of Supervisor is simple:
$ sudo aptitude install supervisor
When Supervisor is installed you can give it programs to start and watch by creating configuration files in the /etc/supervisor/conf.d directory. For our hello-celery worker we’ll create a file named /etc/supervisor/conf.d/hello-celery.conf with this content:
/etc/supervisor/conf.d/hello-celery.conf 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
	[program:hello-celery]
command=/webapps/hello_django/bin/celery --app=hello.celery:app worker --loglevel=INFO
directory=/webapps/hello_django/hello
user=hello
numprocs=1
stdout_logfile=/webapps/hello_django/logs/celery-worker.log
stderr_logfile=/webapps/hello_django/logs/celery-worker.log
autostart=true
autorestart=true
startsecs=10

; Need to wait for currently executing tasks to finish at shutdown.
; Increase this if you have very long running tasks.
stopwaitsecs = 600

; When resorting to send SIGKILL to the program to terminate it
; send SIGKILL to its whole process group instead,
; taking care of its children as well.
killasgroup=true

; if rabbitmq is supervised, set its priority higher
; so it starts first
priority=998

This configuration is based on a sample config provided by the makers of Celery. You can set many other options.
Create a file to store your application’s log messages:
hello@django:~$ mkdir -p /webapps/hello_django/logs/
hello@django:~$ touch /webapps/hello_django/logs/celery-worker.log
After you save the configuration file for your program you can ask supervisor to reread configuration files and update (which will start your the newly registered app).
$ sudo supervisorctl reread
hello-celery: available
$ sudo supervisorctl update
hello-celery: added process group
You can now monitor output of Celery workers by following the celery-worker.log file:
$ tail -f /webapps/hello_django/logs/celery-worker.log
You can also check the status of Celery or start, stop or restart it using supervisor.
$ sudo supervisorctl status hello                       
hello                            RUNNING    pid 18020, uptime 0:00:50
$ sudo supervisorctl stop hello  
hello: stopped
$ sudo supervisorctl start hello                        
hello: started
$ sudo supervisorctl restart hello 
hello: stopped
hello: started
Celery workers should now be automatically started after a system reboot and automatically restarted if they ever crashed for some reason.
Inspecting worker tasks
You can check that Celery is running by issuing the celery status command:
$ export PYTHONPATH=/webapps/hello_django/hello:$PYTHONPATH
$ /webapps/hello_django/bin/celery --app=hello.celery:app status
celery@django: OK

1 node online.
You can also inspect the queue using a friendly curses monitor:
$ export PYTHONPATH=/webapps/hello_django/hello:$PYTHONPATH
$ /webapps/hello_django/bin/celery --app=hello.celery:app control enable_events
$ /webapps/hello_django/bin/celery --app=hello.celery:app events
 
Celery Worker monitor
I hope that’s enough to get you started. You should probably read the Celery User Guide now. Happy coding!

https://realpython.com/blog/python/caching-in-django-with-redis/
Getting Started
We have created an example application to introduce you to the concept of caching. Our application uses:
•	Django (v1.9.8)
•	Django Debug Toolbar (v1.4)
•	django-redis (v4.4.3)
•	Redis (v3.2.0)
Install the App
Visit the admin page in the browser to confirm that the data has been properly loaded.
1
2
3
4
5
	(django-redis)$ python manage.py makemigrations cookbook
(django-redis)$ python manage.py migrate
(django-redis)$ python manage.py createsuperuser
(django-redis)$ python manage.py loaddata cookbook/fixtures/cookbook.json
(django-redis)$ python manage.py runserver

Once you have the Django app running, move onto the Redis installation.
Install Redis
Once installed, Run the Redis server from a new terminal window.
$ redis-server
	
Next, start up the Redis command-line interface (CLI) in a different terminal window and test that it connects to the Redis server. We will be using the Redis CLI to inspect the keys that we add to the cache.

$ redis-cli ping
PONG

Redis provides an API with various commands that a developer can use to act on the data store. Django uses django-redis to execute commands in Redis.

Looking at our example app in a text editor, we can see the Redis configuration in the settings.py file. We define a default cache with the CACHES setting, using a built-in django-redis cache as our backend. Redis runs on port 6379 by default, and we point to that location in our setting. One last thing to mention is that django-redis appends key names with a prefix and a version to help distinguish similar keys. In this case, we have defined the prefix to be “example”.
1
2
3
4
5
6
7
8
9
10
	CACHES = {
    "default": {
        "BACKEND": "django_redis.cache.RedisCache",
        "LOCATION": "redis://127.0.0.1:6379/1",
        "OPTIONS": {
            "CLIENT_CLASS": "django_redis.client.DefaultClient"
        },
        "KEY_PREFIX": "example"
    }
}

NOTE: Although we have configured the cache backend, none of the view functions have implemented caching.
App Performance
As we mentioned at the beginning of this tutorial, everything that the server does to process a request slows the application load time. The processing overhead of running business logic and rendering templates can be significant. Network latency affects the time it takes to query a database. These factors come into play every time a client sends an HTTP request to the server. When users are initiating many requests per second, the effects on performance become noticeable as the server works to process them all.
When we implement caching, we let the server process a request once and then we store it in our cache. As requests for the same URL are received by our application, the server pulls the results from the cache instead of processing them anew each time. Typically, we set a time to live on the cached results, so that the data can be periodically refreshed, which is an important step to implement in order to avoid serving stale data.
You should consider caching the result of a request when the following cases are true:
•	rendering the page involves a lot of database queries and/or business logic,
•	the page is visited frequently by your users,
•	the data is the same for every user,
•	and the data does not change often.
Start by Measuring Performance
Begin by testing the speed of each page in your application by benchmarking how quickly your application returns a response after receiving a request.
To achieve this, we’ll be blasting each page with a burst of requests using loadtest, an HTTP load generator, and then paying close attention to the request rate. Visit the link above to install. Once installed, test the results against the /cookbook/ URL path:
1
	$ loadtest -n 100 -k  http://localhost:8000/cookbook/

Notice that we are processing about 16 requests per second:
1
	Requests per second: 16

When we look at what the code is doing, we can make decisions on how to make changes to improve the performance. The application makes 3 network calls to a database with each request to /cookbook/, and it takes time for each call to open a connection and execute a query. Visit the /cookbook/ URL in your browser and expand the Django Debug Toolbar tab to confirm this behavior. Find the menu labeled “SQL” and read the number of queries:
 

cookbook/services.py
1
2
3
4
5
6
7
	from cookbook.models import Recipe


def get_recipes():
    # Queries 3 tables: cookbook_recipe, cookbook_ingredient,
    # and cookbook_food.
    return list(Recipe.objects.prefetch_related('ingredient_set__food'))

cookbook/views.py
1
2
3
4
5
6
7
8
	from django.shortcuts import render
from cookbook.services import get_recipes


def recipes_view(request):
    return render(request, 'cookbook/recipes.html', {
        'recipes': get_recipes()
    })

The application also renders a template with some potentially expensive logic.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
	<html>
<head>
  <title>Recipes</title>
</head>
<body>
{% for recipe in recipes %}
  <h1>{{ recipe.name }}</h1>
    <p>{{ recipe.desc }}</p>
  <h2>Ingredients</h2>
  <ul>
    {% for ingredient in recipe.ingredient_set.all %}
    <li>{{ ingredient.desc }}</li>
    {% endfor %}
  </ul>
  <h2>Instructions</h2>
    <p>{{ recipe.instructions }}</p>
{% endfor %}
</body>
</html>

Implement Caching
Imagine the total number of network calls that our application will make as users start to visit our site. If 1,000 users hit the API that retrieves cookbook recipes, then our application will query the database 3,000 times and a new template will be rendered with each request. That number only grows as our application scales. Luckily, this view is a great candidate for caching.
The recipes in a cookbook rarely change, if ever. Also, since viewing cookbooks is the central theme of the app, the API retrieving the recipes is guaranteed to be called frequently.
In the example below, we modify the view function to use caching. When the function runs, it checks if the view key is in the cache. If the key exists, then the app retrieves the data from the cache and returns it. If not, Django queries the database and then stashes the result in the cache with the view key. The first time this function is run, Django will query the database and render the template, and then will also make a network call to Redis to store the data in the cache. Each subsequent call to the function will completely bypass the database and business logic and will query the Redis cache.
example/settings.py
1
2
	# Cache time to live is 15 minutes.
CACHE_TTL = 60 * 15

cookbook/views.py
1
2
3
4
5
6
7
8
9
10
11
12
13
14
	from django.conf import settings
from django.core.cache.backends.base import DEFAULT_TIMEOUT
from django.shortcuts import render
from django.views.decorators.cache import cache_page
from cookbook.services import get_recipes

CACHE_TTL = getattr(settings, 'CACHE_TTL', DEFAULT_TIMEOUT)


@cache_page(CACHE_TTL)
def recipes_view(request):
    return render(request, 'cookbook/recipes.html', {
        'recipes': get_recipes()
    })

Notice that we have added the @cache_page() decorator to the view function, along with a time to live. Visit the /cookbook/ URL again and examine the Django Debug Toolbar. We see that 3 database queries are made and 3 calls are made to the cache in order to check for the key and then to save it. Django saves two keys (1 key for the header and 1 key for the rendered page content). Reload the page and observe how the page activity changes. The second time around, 0 calls are made to the database and 2 calls are made to the cache. Our page is now being served from the cache!
When we re-run our performance tests, we see that our application is loading faster.
1
	$ loadtest -n 100 -k  http://localhost:8000/cookbook/

Caching improved the total load, and we are now resolving 21 requests per second, which is 5 more than our baseline:
1
	Requests per second: 21

Inspecting Redis with the CLI
At this point we can use the Redis CLI to look at what gets stored on the Redis server. In the Redis command-line, enter the keys * command, which returns all keys matching any pattern. You should see a key called “example:1:views.decorators.cache.cache_page”. Remember, “example” is our key prefix, “1” is the version, and “views.decorators.cache.cache_page” is the name that Django gives the key. Copy the key name and enter it with the get command. You should see the rendered HTML string.
1
2
3
4
5
	$ redis-cli -n 1
127.0.0.1:6379[1]> keys *
1) "example:1:views.decorators.cache.cache_header"
2) "example:1:views.decorators.cache.cache_page"
127.0.0.1:6379[1]> get "example:1:views.decorators.cache.cache_page"

NOTE: Run the flushall command on the Redis CLI to clear all of the keys from the data store. Then, you can run through the steps in this tutorial again without having to wait for the cache to expire.
Wrap-up
Processing HTTP requests is costly, and that cost adds up as your application grows in popularity. In some instances, you can greatly reduce the amount of processing your server does by implementing caching. This tutorial touched on the basics of caching in Django with Redis, but it only skimmed the surface of a complex topic. Implementing caching in a robust application has many pitfalls and gotchas. Controlling what gets cached and for how long is tough. Cache invalidation is one of the hard things in Computer Science. Ensuring that private data can only be accessed by its intended users is a security issue and must be handled very carefully when caching. Play around with the source code in the example application and as you continue to develop with Django, remember to always keep performance in mind.

https://realpython.com/blog/python/flask-by-example-implementing-a-redis-task-queue/

Flask by Example - Implementing a Redis Task Queue 
This part of the tutorial details how to implement a Redis task queue to handle text processing.
________________________________________
Remember: Here’s what we’re building – A Flask app that calculates word-frequency pairs based on the text from a given URL.

1.	Part One: Set up a local development environment and then deploy both a staging and a production environment on Heroku.
2.	Part Two: Set up a PostgreSQL database along with SQLAlchemy and Alembic to handle migrations.
3.	Part Three: Add in the back-end logic to scrap and then process the word counts from a webpage using the requests, BeautifulSoup, and Natural Language Toolkit (NLTK) libraries.
4.	Part Four: Implement a Redis task queue to handle the text processing. (current)
5.	Part Five: Set up Angular on the front-end to continuously poll the back-end to see if the request is done processing.
6.	Part Six: Push to the staging server on Heroku – setting up Redis and detailing how to run two processes (web and worker) on a single Dyno.
7.	Part Seven: Update the front-end to make it more user-friendly.
8.	Part Eight: Create a custom Angular Directive to display a frequency distribution chart using JavaScript and D3.
Need the code? Grab it from the repo.
Install Requirements
Tools used:
•	Redis (3.0.7)
•	Python Redis (2.10.5)
•	RQ (0.5.6) – a simple library for creating a task queue
Start by downloading and installing Redis from either the official site or via Homebrew (brew install redis). Once installed, start the Redis server:
1
	$ redis-server

Next install Python Redis and RQ in a new terminal window:
1
2
3
	$ cd flask-by-example
$ pip install redis==2.10.5 rq==0.5.6
$ pip freeze > requirements.txt

Set up the Worker
Let’s start by creating a worker process to listen for queued tasks. Create a new file worker.py, and add this code:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
	import os

import redis
from rq import Worker, Queue, Connection

listen = ['default']

redis_url = os.getenv('REDISTOGO_URL', 'redis://localhost:6379')

conn = redis.from_url(redis_url)

if __name__ == '__main__':
    with Connection(conn):
        worker = Worker(list(map(Queue, listen)))
        worker.work()

Here, we listened for a queue called default and established a connection to the Redis server on localhost:6379.
Fire this up in another terminal window:
1
2
3
4
5
	$ cd flask-by-example
$ python worker.py
17:01:29 RQ worker started, version 0.5.6
17:01:29
17:01:29 *** Listening on default...

Now we need to update our app.py to send jobs to the queue…
Update app.py
Add the following imports to app.py:
1
2
3
	from rq import Queue
from rq.job import Job
from worker import conn

Then update the configuration section:
1
2
3
4
5
6
7
8
	app = Flask(__name__)
app.config.from_object(os.environ['APP_SETTINGS'])
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = True
db = SQLAlchemy(app)

q = Queue(connection=conn)

from models import *

q = Queue(connection=conn) set up a Redis connection and initialized a queue based on that connection.
Move the text processing functionality out of our index route and into a new function called count_and_save_words(). This function accepts one argument, a URL, which we will pass to it when we call it from our index route.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
	def count_and_save_words(url):

    errors = []

    try:
        r = requests.get(url)
    except:
        errors.append(
            "Unable to get URL. Please make sure it's valid and try again."
        )
        return {"error": errors}

    # text processing
    raw = BeautifulSoup(r.text).get_text()
    nltk.data.path.append('./nltk_data/')  # set the path
    tokens = nltk.word_tokenize(raw)
    text = nltk.Text(tokens)

    # remove punctuation, count raw words
    nonPunct = re.compile('.*[A-Za-z].*')
    raw_words = [w for w in text if nonPunct.match(w)]
    raw_word_count = Counter(raw_words)

    # stop words
    no_stop_words = [w for w in raw_words if w.lower() not in stops]
    no_stop_words_count = Counter(no_stop_words)

    # save the results
    try:
        result = Result(
            url=url,
            result_all=raw_word_count,
            result_no_stop_words=no_stop_words_count
        )
        db.session.add(result)
        db.session.commit()
        return result.id
    except:
        errors.append("Unable to add item to database.")
        return {"error": errors}


@app.route('/', methods=['GET', 'POST'])
def index():
    results = {}
    if request.method == "POST":
        # get url that the person has entered
        url = request.form['url']
        if 'http://' not in url[:7]:
            url = 'http://' + url
        job = q.enqueue_call(
            func=count_and_save_words, args=(url,), result_ttl=5000
        )
        print(job.get_id())

    return render_template('index.html', results=results)

Take note of the following code:
1
2
3
4
	job = q.enqueue_call(
    func=count_and_save_words, args=(url,), result_ttl=5000
)
print(job.get_id())

Here we used the queue that we initialized earlier and called the enqueue_call() function. This added a new job to the queue and that job ran the count_and_save_words() function with the URL as the argument. The result_ttl=5000 line argument tells RQ how long to hold on to the result of the job for – 5,000 seconds, in this case. Then we outputted the job id to the terminal. This id is needed to see if the job is done processing.
Let’s setup a new route for that…
Get Results
1
2
3
4
5
6
7
8
9
	@app.route("/results/<job_key>", methods=['GET'])
def get_results(job_key):

    job = Job.fetch(job_key, connection=conn)

    if job.is_finished:
        return str(job.result), 200
    else:
        return "Nay!", 202

Let’s test this out.
Fire up the server, navigate to http://localhost:5000/, use the URL http://realpython.com, and grab the job id from the terminal. Then use that id in the ‘/results/’ endpoint – i.e., http://localhost:5000/results/ef600206-3503-4b87-a436-ddd9438f2197.
As long as less than 5,000 seconds have elapsed before you check the status, then you should see an id number, which is generated when we add the results to the database:
1
2
3
4
5
6
7
8
9
10
11
	# save the results
try:
    from models import Result
    result = Result(
        url=url,
        result_all=raw_word_count,
        result_no_stop_words=no_stop_words_count
    )
    db.session.add(result)
    db.session.commit()
    return result.id

Now, let’s refactor the route slightly to return the actual results from the database in JSON:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
	@app.route("/results/<job_key>", methods=['GET'])
def get_results(job_key):

    job = Job.fetch(job_key, connection=conn)

    if job.is_finished:
        result = Result.query.filter_by(id=job.result).first()
        results = sorted(
            result.result_no_stop_words.items(),
            key=operator.itemgetter(1),
            reverse=True
        )[:10]
        return jsonify(results)
    else:
        return "Nay!", 202

Make sure to add the import:
1
	from flask import jsonify

Test this out again. If all went well, you should see something similar to in your browser:
1
2
3
4
5
6
7
8
9
10
11
12
	{
  Course: 5,
  Python: 19,
  Real: 11,
  course: 4,
  courses: 7,
  development: 7,
  product: 4,
  sample: 4,
  videos: 5,
  web: 12
}

http://www.bogotobogo.com/python/python_redis_with_python.php
To create a connection to Redis using redis-py:
$ python
Python 3.4.3 (default, Oct 14 2015, 20:28:29) 
[GCC 4.8.4] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import redis
>>> r = redis.Redis(host='localhost', port=6379, db=0)
The port=6379 and db=0 are default values.



Reading and Writing Data
Now that we are connected to Redis, we can start reading and writing data.
The following code snippet writes the value bar to the Redis key foo, reads it back, and prints it: 
>>> r.set('foo','bar')
True
>>> r.get('foo')
b'bar
The set key to hold the string value. If key already holds a value, it is overwritten, regardless of its type. Any previous time to live associated with the key is discarded on successful SET operation.



incr/decr
The incr/decr increments/decrements the number stored at key by one.
If the key does not exist, it is set to 0 before performing the operation.
>>> r.set('count',1)
True
>>> r.incr('count')
2
>>> r.incr('count')
3
>>> r.decr('count')
2
>>> r.get('count')
b'2'
Error case:
>>> r.set('count',123456789012345678901234567890)
True
>>> r.incr('count')
redis.exceptions.ResponseError: value is not an integer or out of range



rpush, llen, and lindex
The rpush inserts all the specified values at the tail of the list stored at key.
The llen returns the length of the list stored at key.
The lindex returns the element at index index in the list stored at key. The index is zero-based, so 0 means the first element, 1 the second element and so on. 
>>> r.rpush('hispanic', 'uno')
1
>>> r.rpush('hispanic', 'dos')
2
>>> r.rpush('hispanic', 'tres')
3
>>> r.rpush('hispanic', 'cuatro')
4
>>> r.llen('hispanic')
4
>>> r.lindex('hispanic', 3)
b'cuatro'


You mean other than the fact that Redis is a key-value database and RabbitMQ is a messaging system? Seriously though... I don't know what RestMQ is, but if it is something that sits on top of Redis, it is something that is trying to be something that Redis isn't. RabbitMQ on the other hand is arguably the best message-oriented-middleware on the planet. 
While I am also using RabbitMQ quite happily, I'm currently exploring a Redis broker, since the AMQP protocol is likely overkill for my logging use case.
We are defining an architecture to collect log information by Logstash shippers which are installed in various machines and index the data in one elasticsearch server centrally and use Kibana as the graphical layer. We need a reliable messaging system in between Logstash shippers and elasticsearch to grantee the delivery. What factors should be considered when selecting Redis over RabbitMQ as a data broker/messaging system in between Logstash shippers and the elasticsearch or vice versa?

Redis is created as a key value data store despite having some basic message broker capabilities.
RabbitMQ is created as a message broker. It has lots of message broker capabilities naturally.
I have been doing some research on this topic. If performance is important and persistence is not, RabbitMQ is a perfect choice. Redis is a technology developed with a different intent. 
Following are my list of pros for using RabbitMQ over Redis:
•	RabbitMQ uses AMQP protocol which can be configured to use SSL, additional layer of security. By using SSL certificates it can encrypt the data that you are sending to the broker and it means that no one will sniff your data and have access to your vital organizational data.
•	RabbitMQ takes approximately 75% of the time Redis takes in accepting messages. 
•	RabbitMQ supports priorities for messages, which can be used by workers to consume high priority messages first. 
•	There is no chance of loosing the message if any worker crashes after consuming the message, which is not the case with Redis. 
•	RabbitMQ has a good routing system to direct messages to different queues.
•	Regarding scaling, RabbitMQ has a built in cluster implementation that you can use in addition to a load balancer in order to implement a redundant broker environment.
A few cons for using RabbitMQ:
•	RabbitMQ might be a little hard to maintain, hard to debug crashes. 
•	node-name or node-ip fluctuations can cause data loss, but if managed well, durable messages can solve the problem.
While I am also using RabbitMQ quite happily, I'm currently exploring a Redis broker, since the AMQP protocol is likely overkill for my logging use case.

https://www.rabbitmq.com/features.html
What can RabbitMQ do for you?
Messaging enables software applications to connect and scale. Applications can connect to each other, as components of a larger application, or to user devices and data. Messaging is asynchronous, decoupling applications by separating sending and receiving data. 
You may be thinking of data delivery, non-blocking operations or push notifications. Or you want to use publish / subscribe, asynchronous processing, or work queues. All these are patterns, and they form part of messaging. 
RabbitMQ is a messaging broker - an intermediary for messaging. It gives your applications a common platform to send and receive messages, and your messages a safe place to live until received. 
Feature Highlights
Reliability
RabbitMQ offers a variety of features to let you trade off performance with reliability, including persistence, delivery acknowledgements, publisher confirms, and high availability. 
Flexible Routing
Messages are routed through exchanges before arriving at queues. RabbitMQ features several built-in exchange types for typical routing logic. For more complex routing you can bind exchanges together or even write your own exchange type as a plugin. 
Clustering
Several RabbitMQ servers on a local network can be clustered together, forming a single logical broker. 
Federation
For servers that need to be more loosely and unreliably connected than clustering allows, RabbitMQ offers a federation model. 
Highly Available Queues
Queues can be mirrored across several machines in a cluster, ensuring that even in the event of hardware failure your messages are safe. 
Multi-protocol
RabbitMQ supports messaging over a variety of messaging protocols. 
Many Clients
There are RabbitMQ clients for almost any language you can think of. 
Management UI
RabbitMQ ships with an easy-to use management UI that allows you to monitor and control every aspect of your message broker. 
Tracing
If your messaging system is misbehaving, RabbitMQ offers tracing support to let you find out what's going on. 
Plugin System
RabbitMQ ships with a variety of plugins extending it in different ways, and you can also write your own. 
And Also...
Commercial Support
Commercial support, training and consulting are available from Pivotal. 
Large Community
There's a large community around RabbitMQ, producing all sorts of clients, plugins, guides, etc. Join our mailing list to get involved! 


https://spring.io/understanding/AMQP
l Understanding AMQP
AMQP (Advanced Message Queueing Protocol) is an openly published wire specification for asynchronous messaging. Every byte of transmitted data is specified. This characteristic allows libraries to be written in many languages, and to run on multiple operating systems and CPU architectures, which makes for a truly interoperable, cross-platform messaging standard.
l Advantages of AMQP over JMS
AMQP is often compared to JMS (Java Message Service), the most common messaging system in the Java community. A limitation of JMS is that the APIs are specified, but the message format is not. Unlike AMQP, JMS has no requirement for how messages are formed and transmitted. Essentially, every JMS broker can implement the messages in a different format. They just have to use the same API.
Thus Pivotal has released a JMS on Rabbit project, a library that implements the JMS APIs but uses RabbitMQ, an AMQP broker, to transmit the messages.
AMQP publishes its specifications in a downloadable XML format. This availability makes it easy for library maintainers to generate APIs driven by the specs while also automating construction of algorithms to marshal and demarshal messages.
These advantages and the openness of the spec have inspired the creation of multiple brokers that support AMQP, including:
•	RabbitMQ 
•	ActiveMQ 
•	Qpid 
•	Solace
l AMQP and JMS terminology
•	JMS has queues and topics. A message sent on a JMS queue is consumed by no more than one client. A message sent on a JMS topic may be consumed by multiple consumers. AMQP only has queues. While AMQP queues are only consumed by a single receiver, AMQP producers don't publish directly to queues. A message is published to an exchange, which through its bindings may get sent to one queue or multiple queues, effectively emulating JMS queues and topics. 
•	JMS and AMQP have an equivalent message header, providing the means to sort and route messages. 
•	JMS and AMQP both have brokers responsible for receiving, routing, and ultimately dispensing messages to consumers. 
•	AMQP has exchanges, routes, and queues. Messages are first published to exchanges. Routes define on which queue(s) to pipe the message. Consumers subscribing to that queue then receive a copy of the message. If more than one consumer subscribes to the same queue, the messages are dispensed in a round-robin fashion.

https://blog.docker.com/2016/03/containers-are-not-vms/

I spend a good portion of my time at Docker talking to community members with varying degrees of familiarity with Docker and I sense a common theme: people’s natural response when first working with Docker is to try and frame it in terms of virtual machines. I can’t count the number of times I have heard Docker containers described as “lightweight VMs”
.
I get it because I did the exact same thing when I first started working with Docker. It’s easy to connect those dots as both technologies share some characteristics. Both are designed to provide an isolated environment in which to run an application. Additionally, in both cases that environment is represented as a binary artifact that can be moved between hosts. There may be other similarities, but to me these are the two biggies.

The key is that the underlying architecture is fundamentally different between the two. The analogy I use (because if you know me, you know I love analogies) is comparing houses (VMs) to apartment buildings (containers).

Houses (the VMs) are fully self-contained and offer protection from unwanted guests. They also each possess their own infrastructure – plumbing, heating, electrical, etc. Furthermore, in the vast majority of cases houses are all going to have at a minimum a bedroom, living area, bathroom, and kitchen. I’ve yet to ever find a “studio house” – even if I buy the smallest house I may end up buying more than I need because that’s just how houses are built.  (for the pedantic out there, yes I’m ignoring the new trend in micro houses because they break my analogy).

Apartments (the containers) also offer protection from unwanted guests, but they are built around shared infrastructure. The apartment building (Docker Host) shares plumbing, heating, electrical, etc. Additionally apartments are offered in all kinds of different sizes – studio to multi-bedroom penthouse. You’re only renting exactly what you need. Finally, just like houses, apartments have front doors that lock to keep out unwanted guests.
With containers, you share the underlying resources of the Docker host and you build an image that is exactly what you need to run your application. You start with the basics and you add what you need. VMs are built in the opposite direction. You are going to start with a full operating system and, depending on your application, might be strip out the things you don’t want.
I’m sure many of you are saying “yeah, we get that. They’re different”. But even as we say this, we still try and adapt our current thoughts and processes around VMs and apply them to containers.
•	“How do I backup a container?”
•	“What’s my patch management strategy for my running containers?”
•	“Where does the application server run?”
To me the light bulb moment came when I realized that Docker is not a virtualization technology, it’s an application delivery technology. In a VM-centered world, the unit of abstraction is a monolithic VM that stores not only application code, but often its stateful data. A VM takes everything that used to sit on a physical server and just packs it into a single binary so it can be moved around.  But it is still the same thing.  With containers the abstraction is the application; or more accurately a service that helps to make up the application.
With containers, typically many services (each represented as a single container) comprise an application. Applications are now able to be deconstructed into much smaller components which fundamentally changes the way they are managed in production.
So, how do you backup your container, you don’t. Your data doesn’t live in the container, it lives in a named volume that is shared between 1-N containers that you define. You backup the data volume, and forget about the container. Optimally your containers are completely stateless and immutable.
Certainly patches will still be part of your world, but they aren’t applied to running containers. In reality if you patched a running container, and then spun up new ones based on an unpatched image, you’re gonna have a bad time. Ideally you would update your Docker image, stop your running containers, and fire up new ones. Because a container can be spun up in a fraction off a second, it’s just much cheaper to go this route.
Your application server translates into a service run inside of a container. Certainly there may be cases where your microservices-based application will need to connect to a non-containerized service, but for the most part standalone servers where you execute your code give way to one or more containers that provide the same functionality with much less overhead (and offer up much better horizontal scaling).
“But, VMs have traditionally been about lift and shift. What do I do with my existing apps?”
I often have people ask me how to run huge monolithic apps in a container. There are many valid strategies for migrating to a microservices architecture that start with moving an existing monolithic application from a VM into a container but that should be thought of as the first step on a journey, not an end goal.
As you consider how your organization can leverage Docker, try and move away from a VM-focused mindset and realize that Docker is way more than just “a lightweight VM.” It’s an application-centric way to  deliver high-performing, scalable applications on the infrastructure of your choosing.





RQ (Redis Queue) is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis and it is designed to have a low barrier to entry. It can be integrated in your web stack easily. RQ requires Redis >= 2.6.0.

What is a session cache? 
Session Cache is a caching service that stores and persists HTTP session objects to a remote data grid. The data grid is a general use grid that stores strings and objects. Session Cache remotely leverages the caching capabilities of the data grid and lets you store session data.

LRU is a cache eviction algorithm called least recently used cache. Look at this resource. LFU is a cache eviction algorithm called least frequently used cache. It requires three data structures. One is a hash table which is used to cache the key/values so that given a key we can retrieve the cache entry at O(1).


The logic for implementing any cache is the following:
Check if object exists in cache by fetching it.
If it doesn't exist, calculate the object (or generate it) and put it in the cache.
Return the object.
Django provides a simple dictionary-like API for caches. Once you have correctly configured the cache, you can use the simple cache api:
from django.core.cache import cache

def get(request):
   value = cache.get('somekey')
   if not value:
      # The value in the cache for the key 'somekey' has expired
      # or doesn't exist, so we generate the value
      value = 42
      cache.set('somekey', value)


What is celery used for Python? 
Celery is an asynchronous task queue/job queue based on distributed message passing. It is focused on real-time operation, but supports scheduling as well. The execution units, called tasks, are executed concurrently on a single or more worker servers using multiprocessing, Eventlet, or gevent.

What is the difference between cookies and session? 
The main difference between a session and a cookie is that session data is stored on the server, whereas cookies store data in the visitor's browser. Sessions are more secure than cookies as it is stored in server.Cookie can be turned off from browser. Jun 14, 2011

What is the difference between a cookie and a cache? 
Difference between Cache and Cookies. Cookie is a very small piece of information that is stored on the client's machine by the web site and is sent back to the server each time a page is requested. ... Cache is a temporary storage of web page resources stored on client's machine for quicker loading of the web pages.

What is Elasticache Amazon? 
Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud. ... ElastiCache for Redis is fully managed, scalable, and secure - making it an ideal candidate to power high-performance use cases such as Web, Mobile Apps, Gaming, Ad-Tech, and IoT.

Why do we need memcached? 
Memcached is not typically used on the same machine as the application server, the reason for this is because it is designed to be used via TCP (it would be accessible via sockets if it were designed to work on the same server) and it was designed as a pooling server.May 26, 2012

Is Redis distributed? 
A single instance is therefore limited by the maximum memory of your server. Now, you can also share the data to several Redis instances, running on multiple servers. ... Regarding question 2, a single Redis instance is not a distributed system. It is a remote centralized store.

What is meant by LRU Least Recently Used? 
Least recently used. (Operating systems) (LRU) A rule used in a paging system which selects a page to be paged out if it has been used (read or written) less recently than any other page. The same rule may also be used in a cache to select which cache entry to flush.
What is the LRU cache? 
Writing an LRU Cache. ... It's a cache that, when low on memory, evicts least recently used items. LRU is an eviction policy that makes a lot of sense for the typical kind of cache we all deal with on a daily basis

Let's consider a constant stream of cache requests with a cache capacity of 3, see below: 
A, B, C, A, A, A, A, A, A, A, A, A, A, A, B, C, D
If we just consider a Least Recently Used (LRU) cache with a HashMap + doubly linked list implementation with O(1) eviction time and O(1) load time, we would have the following elements cached while processing the caching requests as mentioned above. 
[A]
[A, B]
[A, B, C]
[B, C, A] <- a stream of As keeps A at the head of the list.
[C, A, B]
[A, B, C]
[B, C, D] <- here, we evict A, we can do better! 
When you look at this example, you can easily see that we can do better - given the higher expected chance of requesting an A in the future, we should not evict it even if it was least recently used. 
A - 12
B - 2
C - 2
D - 1
Least Frequently Used (LFU) cache takes advantage of this information by keeping track of how many times the cache request has been used in its eviction algorithm.
It requires three data structures. One is a hash table which is used to cache the key/values so that given a key we can retrieve the cache entry at O(1). Second one is a double linked list for each frequency of access. The max frequency is capped at the cache size to avoid creating more and more frequency list entries. If we have a cache of max size 4 then we will end up with 4 different frequencies. Each frequency will have a double linked list to keep track of the cache entries belonging to that particular frequency. The third data structure would be to somehow link these frequencies lists. It can be either an array or another linked list so that on accessing a cache entry it can be easily promoted to the next frequency list in time O(1).
The main difference is that in LRU we only check on which page is recently that used old in time than other pages i.e checking only based on recent used pages. BUT in LFU we check the old page as well as the frequency of that page and if frequency of the page is lager than the old page we cant remove it and if we all old pages are having same frequency then take last i.e FIFO method for that. and remove page....

https://realpython.com/blog/python/asynchronous-tasks-with-django-and-celery/

Q. What is Amazon EC2 service?

Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable (scalable) computing capacity in the cloud. You can use Amazon EC2 to launch as many virtual servers you need. In Amazon EC2 you can configure security and networking as well as manage storage.Amazon EC2 service also helps in obtaining and configuring capacity using minimal friction.

Q. What is the difference between Scalability and Elasticity?

Scalability is the ability of a system to increase its hardware resources to handle the increase in demand. It can be done by increasing the hardware specifications or increasing the processing nodes.

Elasticity is the ability of a system to handle increase in the workload by adding additional hardware resources when the demand increases(same as scaling) but also rolling back the scaled resources, when the resources are no longer needed. This is particularly helpful in Cloud environments, where a pay per use model is followed.


Q. What are the features of the Amazon EC2 service?

As the Amazon EC2 service is a cloud service so it has all the cloud features. Amazon EC2 provides the following features:

Virtual computing environment (known as instances)

Pre-configured templates for your instances (known as Amazon Machine Images – AMIs)

Amazon Machine Images (AMIs) is a complete package that you need for your server (including the operating system and additional software)

Amazon EC2 provides various configurations of CPU, memory, storage and networking capacity for your instances (known as instance type)

Secure login information for your instances using key pairs (AWS stores the public key and you can store the private key in a secure place)

Storage volumes of temporary data is deleted when you stop or terminate your instance (known as instance store volumes)

Amazon EC2 provides persistent storage volumes (using Amazon Elastic Block Store – EBS)

A firewall that enables you to specify the protocols, ports, and source IP ranges that can reach your instances using security groups

Static IP addresses for dynamic cloud computing (known as Elastic IP address)

Amazon EC2 provides metadata (known as tags)

Amazon EC2 provides virtual networks that are logically isolated from the rest of the AWS cloud, and that you can optionally connect to your own network (known as virtual private clouds – VPCs)


Q. What is Amazon Machine Image and what is the relation between Instance and AMI?

Amazon Web Services provides several ways to access Amazon EC2, like web-based interface, AWS Command Line Interface (CLI) and Amazon Tools for Windows Powershell. First, you need to sign up for an AWS account and you can access Amazon EC2.
 Amazon EC2 provides a Query API. These requests are HTTP or HTTPS requests that use the HTTP verbs GET or POST and a Query parameter named Action.
Interested in mastering AWS Training? Enroll now for FREE demo on AWS Training.


Q. What is Amazon Machine Image (AMI)?

An Amazon Machine Image (AMI) is a template that contains a software configuration (for example, an operating system, an application server, and applications). From an AMI, we launch an instance, which is a copy of the AMI running as a virtual server in the cloud. We can even launch multiple instances of an AMI.


Q. What is the relation between Instance and AMI?

We can launch different types of instances from a single AMI. An instance type essentially determines the hardware of the host computer used for your instance. Each instance type offers different compute and memory capabilities.

After we launch an instance, it looks like a traditional host, and we can interact with it as we would do with any computer. We have complete control of our instances; we can use sudo to run commands that require root privileges.


Q. Explain storage for Amazon EC2 instance.

Amazon EC2 provides many data storage options for your instances. Each option has a unique combination of performance and durability. These storages can be used independently or in combination to suit your requirements.

There are mainly four types of storages provided by AWS.

Amazon EBS: Its durable, block-level storage volumes  can be attached in running Amazon EC2 instance. The Amazon EBS volume persists independently from the running life of an Amazon EC2 instance. After an EBS volume is attached to an instance, you can use it like any other physical hard drive. Amazon EBS encryption feature supports encryption feature.
Amazon EC2 Instance Store: Storage disk that is attached to the host computer is referred to as instance store. The instance storage provides temporary block-level storage for Amazon EC2 instances. The data on an instance store volume persists only during the life of the associated Amazon EC2 instance; if you stop or terminate an instance, any data on instance store volumes is lost.
Amazon S3: Amazon S3 provides access to reliable and inexpensive data storage infrastructure. It is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within Amazon EC2 or anywhere on the web.
Adding Storage: Every time you launch an instance from an AMI, a root storage device is created for that instance. The root storage device contains all the information necessary to boot the instance. You can specify storage volumes in addition to the root device volume when you create an AMI or launch an instance using block device mapping.


Q. What are the Security Best Practices for Amazon EC2?

There are several best practices for secure Amazon EC2. Following are few of them.


Use AWS Identity and Access Management (AM) to control access to your AWS resources.

Restrict access by only allowing trusted hosts or networks to access ports on your instance.

Review the rules in your security groups regularly, and ensure that you apply the principle of least

Privilege — only open up permissions that you require.

Disable password-based logins for instances launched from your AMI. Passwords can be found or cracked, and are a security risk.


Q. Explain Stopping, Starting, and Terminating an Amazon EC2 instance?

Stopping and Starting an instance: When an instance is stopped, the instance performs a normal shutdown and then transitions to a stopped state. All of its Amazon EBS volumes remain attached, and you can start the instance again at a later time. You are not charged for additional instance hours while the instance is in a stopped state.
Terminating an instance: When an instance is terminated, the instance performs a normal shutdown, then the attached Amazon EBS volumes are deleted unless the volume’s deleteOnTermination attribute is set to false. The instance itself is also deleted, and you can’t start the instance again at a later time.


Q. Explain Elastic Block Storage?  What type of performance can you expect?  How do you back it up?  How do you improve performance?

EBS is a virtualized SAN or storage area network.  That means it is RAID storage to start with, so it’s redundant and fault tolerant.  If disks die in that RAID you don’t lose data.  Great!  It is also virtualized, so you can provision and allocate storage, and attach it to your server with various API calls.  No calling the storage expert and asking him or her to run specialized commands from the hardware vendor.

Performance on EBS can exhibit variability.  That is, it can go above the SLA performance level, then drop below it.  The SLA provides you with an average disk I/O rate you can expect.  This can frustrate some folks, especially performance experts who expect reliable and consistent disk throughout on a server.  Traditional physically hosted servers behave that way.  Virtual AWS instances do not.

Backup EBS volumes by using the snapshot facility via API call or via a GUI interface like elasticfox.

Improve performance by using Linux software raid and striping across four volumes.


Q. What is S3?  What is it used for?  Should encryption be used?

S3 stands for Simple Storage Service.  You can think of it like FTP storage, where you can move files to and from there, but not mount it like a filesystem.  AWS automatically puts your snapshots there, as well as AMIs there.  Encryption should be considered for sensitive data, as S3 is a proprietary technology developed by Amazon themselves, and as yet unproven vis-a-vis a security standpoint.


Q. What is an AMI?  How do I build one?

AMI stands for Amazon Machine Image.  It is effectively a snapshot of the root filesystem.  Commodity hardware, servers have a bios that points the master boot record of the first block on a disk.  A disk image, though can sit anywhere physically on a disk, so Linux can boot from an arbitrary location on the EBS storage network.

Build a new AMI by first spinning up and instance from a trusted AMI.  Then adding packages and components as required.  Be wary of putting sensitive data onto an AMI.  For instance, your access credentials should be added to an instance after spinup  with a database, mount an outside volume that holds your MySQL data after spinup as well.


Q. Can I vertically scale an Amazon instance?  How?

Yes.  This is an incredible feature of AWS and cloud virtualization.  Spin up a new larger instance than the one you are currently running.  Pause that instance and detach the root ebs volume from this server and discard.  Then stop your live instance, detach its root volume.  Note down the unique device ID and attach that root volume to your new server.   And then start it again.  Voila, you have scaled vertically in-place!!


Q. What is auto-scaling?  How does it work?

Auto-scaling is a feature of AWS which allows you to configure and automatically provision and spin up new instances without the need for your intervention.  You do this by setting thresholds and metrics to monitor.  When those thresholds are crossed, a new instance of your choosing will be spun up, configured, and rolled into the load balancer pool.  Voila, you’ve scaled horizontally without any operator intervention!


Q. What automation tools can I use to spin up servers?

The most obvious way is to roll-your-own scripts, and use the AWS API tools.  Such scripts could be written in bash, Perl or another language or your choice. The next option is to use a configuration management and provisioning tools like puppet or better it’s successor Opscode Chef.  You might also look towards a tool like Scalr.  Lastly, you can go with a managed solution such as Rightscale.


Q. What is configuration management?  Why would I want to use it with cloud provisioning of resources?

Configuration management has been around for a long time in web operations and systems administration.  Yet the cultural popularity of it has been limited.  Most systems administrators configure machines as software was developed before version control – that is manually making changes on servers.  Each server can then and usually is slightly different.  Troubleshooting though, is straightforward as you login to the box and operate on it directly.  Configuration management brings a large automation tool in the picture, managing servers like strings of a puppet.  This forces standardization, best practices, and reproducibility as all configs are versioned and managed.  It also introduces a new way of working which is the biggest hurdle to its adoption.

Enter the cloud, then configuration management becomes even more critical.  That’s because virtual servers such as amazons EC2 instances are much less reliable than physical ones.  You absolutely need a mechanism to rebuild them as-is at any moment.  This pushes best practices like automation, reproducibility and disaster recovery into center stage.


Q. Explain how you would simulate perimeter security using the Amazon Web Services model?

Traditional perimeter security that we’re already familiar with using firewalls and so forth is not supported in the Amazon EC2 world.  AWS supports security groups.  One can create a security group for a jump box with ssh access – only port 22 open.  From there a webserver group and database group are created.  The webserver group allows 80 and 443 from the world, but port 22 *only* from the jump box group.  Further the database group allows port 3306 from the webserver group and port 22 from the jump box group.  Add any machines to the webserver group and they can all hit the database.  No one from the world can, and no one can directly ssh to any of your boxes.


Q. How to use Amazon SQS?

Amazon SQS (Simple Queue Service) is a message passing mechanism that is used for communication between different connectors that are connected with each other. It also acts as a communicator between various components of Amazon. It keeps all the different functional components together. This functionality helps different components to be loosely coupled, and provide an architecture that is more failure resilient system.


