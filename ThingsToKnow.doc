Replacing or substituting string : $sed 's/unix/linux/' geekfile.txt
  
Replacing all the occurrence of the pattern in a line : $sed 's/unix/linux/g' geekfile.txt

Delete Lines:   $ sed '2d' myfile

What about deleting a range of lines?: $ sed '2,3d' myfile
$ sed '3,$d' myfile                     delete to the end. Here we delete from the third line to the end of the file.

Print lines between PAT1 and PAT2 - include PAT1 and PAT2: sed -n '/PAT1/,/PAT2/p' FILE


sed -n 'Np' FILE.txt
Example:
To print 1st line,   $ sed -n '1p' sedtest.txt

This is line #1While, to print last line, $ sed -n '$p' sedtest.txt

To print 3rd line to 8th line.

$ sed -n '3,8p' sedtest.txt
This is line #3
This is line #4
This is line #5
This is line #6
This is line #7
This is line #8

Similarly, in order to print lines starting from 5th up to last line, you would run-

$ sed -n '5,$p' sedtest.txt
This is line #5
This is line #6
This is line #7
This is line #8
This is line #9
This is line #10

How to delete first line and print rest?
docker container ps | awk '{print $1}' | sed 1d			# Use sed to delete first line

How to convert a docx file to md file?
$ pandoc -f docx -t markdown README.docx -o README.md

How to kill all running celery processes found?
kill $(ps aux | grep -v 'grep -i celery' | grep -i celery | awk '{print $2}')

zip -r filename.zip filesdir

Putty session locked up from CTRL+S? How to unlock Putty session?
If you are using putty terminal, putty session locks up and becomes unresponsive after you pressed “CTRL + S”
To restore current putty session, press “CTRL + Q”

CTSL+S => XOFF
CTRL+Q => XON

ss - Another utility to investigate sockets
ss is used to dump socket statistics. It allows showing information similar to netstat.  It can display more TCP and state information than other tools.
sudo (sometimes considered as short for Super-user do)

Oracle specific commands:
To connect Oracle DB in linux:
sqlplus username/password@hostname:portnumber/Servicename
sqlplus user0001/password0001@hostname01.us.companyDomain:20001/DBName

e.g. in our E2 source:
      'ENGINE': 'django.db.backends.oracle',
      'NAME': 'DBName',
      'USER': 'user0001',
      'PASSWORD': 'password0001',
      'HOST': 'hostname01.us.companyDomain',
      'PORT': '20001'

Queries:
select table_name from all_tables
SELECT view_name FROM all_views
select table_name, owner, tablespace_name from all_tables
select * from all_tables
SELECT table_name FROM user_tables

Try the below data dictionary views.
tabs
dba_tables
all_tables
user_tables
Select * from dba_tables; -- need to have select catalog role
Select * from user_tables;-- to see tables of your schema
Select * from all_tables; -- tables inside your schma and tables of orher schema which you possess select grants on

ldd - print shared library dependencies


UDP: 
Anything where you don't care too much if you get all data always
UDP is a connectionless protocol - it simply has a destination and nodes simply pass it along if it comes as best as they can. So packets arriving out of order, along various routes etc are common. So Instant messengers and similar software developers think UDP an ideal solution.
In real life if you want to throw data in the net, without worrying about time taken to reach, order of reaching use UDP.
Tunneling/VPN (lost packets are ok - the tunneled protocol takes care of it)
Streaming media applications such as movies (lost frames are ok)
Online multiplayer games or Games that don't care if you get every update
Local broadcast mechanisms (same application running on different machines "discovering" each other)
Domain Name System (DNS)
Voice over IP (VoIP)
Trivial File Transfer Protocol (TFTP)
TCP + UDP = SMTP (example: mobile, telephone)

* **Reason: ***suppose if some packet (frame/sequence) is missing we can understand the content. Because video is collection of frames. For 1 second video there should be 25 frames (image).Even though we can understand some frames are missing due to our imagination skills. That’s why UDP is used for video streaming.

TCP: 
Almost anything where you have to get all transmitted data
TCP is a connection oriented protocol, it establishes a path, or a virtual connection all the way through switches routers proxies etc. and then starts any communication. Various mechanisms like routing djikstras shortest path algorithm exist to establish the virtual end to end connection. So it finds itself used while browsing HTML and other pages, making payments and web applications in general.
If you want a solid path before you start throwing packets, and want same order and latency for your data packets use TCP - I will use UDP for Torrents and TCP for PayPal!
www or World Wide Web(HTTP)
SSH, FTP, telnet
Email(SMTP, sending mail / IMAP/POP, receiving mail)

UDP is mailing a letter at the post office.
TCP is mailing a letter with a return receipt at the post office, except that the post master will organize the letters in-order-of mailing and only deliver them in-order.

The classic standpoint is to consider TCP as safe and UDP as unreliable.
But when TCP-IP protocols are used in safety critical applications, TCP is not recommended because it can stop on error for multiple reasons. Whereas UDP lets the application software deal with errors, retransmission timers, etc.

Moreover, TCP has more processing overhead than UDP.
Currently, UDP is used in aircraft controls and flight instruments, in the ARINC 664 standard also named AFDX (Avionics Full-Duplex Switched Ethernet). In ARINC 664, TCP is optional but UDP is used with the RTOS (real time operating systems) designed for the ARINC 653 standard (high reliability control software in civil aircrafts).

TCP
I will not send data anymore until i get an acknowledgment.
this process is slow
It is used for security purpose

UDP
Here i have no headache with acknowledgment.
this process is faster but here data can be lost .
example : video streaming , online games etc

Understanding Active Directory services(LDAP):
·	CN = Common Name
·	OU = Organizational Unit
·	DC = Domain Component
These are all parts of the X.500 Directory Specification, which defines nodes in a LDAP directory.
You can also read up on LDAP data Interchange Format (LDIF), which is an alternate format.
You read it from right to left, the right-most component is the root of the tree, and the left most component is the node (or leaf) you want to reach.
Each = pair is a search criteria. 
With your example query
("CN=Dev-India,OU=Distribution Groups,DC=gp,DC=gl,DC=google,DC=com");
In effect the query is:
From the com Domain Component, find the google Domain Component, and then inside it the gl Domain Component and then inside it the gp Domain Component.
In the gp Domain Component, find the Organizational Unit called Distribution Groups and then find the the object that has a common name of Dev-India.
It is a DN (Distinguished Name) (a series of comma-separated key/value pairs used to identify entries uniquely in the directory hierarchy). The DN is actually the entry's fully qualified name.
Here you can see an example where I added some more possible entries.
The actual path is represented using green.
 
The following paths represent DNs (and their value depends on what you want to get after the query is run):
·	"DC=gp,DC=gl,DC=google,DC=com"
·	"OU=Distribution Groups,DC=gp,DC=gl,DC=google,DC=com"
·	"OU=People,DC=gp,DC=gl,DC=google,DC=com"
·	"OU=Groups,DC=gp,DC=gl,DC=google,DC=com"
·	"CN=QA-USA,OU=Distribution Groups,DC=gp,DC=gl,DC=google,DC=com"
·	"CN=Dev-India,OU=Distribution Groups,DC=gp,DC=gl,DC=google,DC=com"
"CN=Ted Owen,OU=People,DC=gp,DC=gl,DC=google,DC=com"
LDAP Namespace Structure 
A directory service has two major features. First, it distributes its information base among many different servers. Second, users can access directory information by querying any of those servers. Making this work requires defining a namespace in which each object's location can be quickly determined. 
Common Names
As we saw in the last section, information in an LDAP database comes in the form of objects. Objects have attributes that describe them. For example, the User object for Tom Jones would have attributes such as Tom's logon name, his password, his phone number, his email address, his department, and so forth. 
When an LDAP client needs to locate information about an object, it submits a query that contains the object's distinguished name (DN) and the attributes the client wants to see. A search for information about Tom Jones could be phrased in a couple of ways: 
·	You could search for attributes in Tom's User object. “Give me the Department attribute for cn=Tom Jones,cn=Users,dc=Company,dc=com.” 
Distinguished Names
A name that includes an object's entire path to the root of the LDAP namespace is called its distinguished name, or DN. An example DN for a user named CSantana whose object is stored in the cn=Users container in a domain named Company.com would be cn=CSantana,cn=Users,dc=Company,dc=com. 
Relative Distinguished Names
An object name without a path, or a partial path, is called a relative distinguished name, or RDN. The common name cn=CSantana is an example of an RDN. So is cn=CSantana,cn=Users. The RDN serves the same purpose as a path fragment in a filename. It is a convenient navigational shortcut. 
Two objects can have the same RDN, but LDAP has a rule that no two objects can have the same DN. This makes sense if you think of the object-oriented nature of the database. Two objects with the same DN would try to occupy the same row in the database table. C'est impossible, as we say in southern New Mexico. 
Case Sensitivity of LDAP Names 
Distinguished names in Active Directory are not case sensitive. In most instances, the case you specify when you enter a value is retained in the object's attribute. This is similar to the way Windows treats filenames. Feel free to mix cases based on your corporate standards or personal aesthetic. 
Typeless Names and Delimiters
If you write scripts and you need to allow for periods in object names, precede the period with a backslash. This tells the parser that the period is a special character, not a delimiter. For example, if your user names look like tom.collins, a typeless name in a script would look like this: tom\.collins.Users.Company.com. The same is true for user names that have embedded commas and periods, such as Winston H. Borntothepurple, Jr. An ADSI query for this name would look like this: winston h\. borntothepurple\, jr\. 
Naming Contexts
As the number of objects in a DIT grows, the database may get too large to store efficiently on one DSA. Also, an organization might want to use bandwidth more effectively by using a DSA in New York to store information about users in North America and another DSA in Amsterdam to store information about users in Europe. 
Naming Contexts and Partitions
X.501, “Information Technology—Open Systems Interconnection—The Directory: Models,” defines the term naming context as, “A subtree of entries held in a single master DSA.” It goes on to describe the process of dividing a tree into multiple naming contexts as partitioning.
Novell chose to adopt the term partition to define separate pieces of the directory database. In their seminal book, Understanding and Deploying LDAP Directory Services, Tim Howe, Mark Smith, and Gordon Good use the term partition in favor of naming context, although they describe both as meaning the same thing. Microsoft uses the two terms interchangeably. 
The tools that come with the Windows Server 2003 CD and in the Resource Kit favor the term naming context. That is the term I use throughout this book. 
Here is where the distributed nature of an LDAP database comes into play. The Directory Information Base can be separated into parts called naming contexts, or NCs. In Active Directory, each domain represents a separate naming context. Domain controllers in the same domain each have a read/write replica of that Domain naming context. Configuration and Schema objects are stored in their own naming contexts, as are DNS Record objects when using Active Directory Integrated DNS zones. 
When a client submits a query for information about a particular object, the system must determine which DSA hosts the naming context that contains that particular object. It does this using the object's distinguished name and knowledge about the directory topology. 
If a DSA cannot respond to a query using information in the naming contexts it hosts, it sends the client a referral to a DSA hosting the next higher or lower naming context in the tree (depending on the distinguished name of the object in the search). The client then submits the request to a DSA hosting the naming context in the referral. This DSA either responds with the information being requested or a referral to another DSA. This is called walking the tree.
DSAs that host copies of the same naming context must replicate changes to each other. It's important to keep this in mind as you work with Active Directory servers. If you have separate domains, then clients in one domain must walk the tree to get access to Active Directory objects in another domain. If the domain controllers for the domains are in different locations in the WAN, this can slow performance. Many of the architectural decisions you'll make as you design your system focus on the location, accessibility, and reliability of naming contexts. 
LDAP Searches 
From a client's perspective, LDAP operates like a well-run department store. In a department store, you can sidle up to the fragrance counter and ask, “How much is the Chanel No. 5?” and be sure of getting an immediate reply, especially if you already have your credit card in hand. The same is true of LDAP. When a search request is submitted to a DSA that hosts a copy of the naming context containing the objects involved in the search, the DSA can answer the request immediately. 
But in a department store, what if you ask the fragrance associate, “Where can I find a size 16 chambray shirt that looks like a Tommy Hilfiger design but doesn't cost so darn much?” The associate probably doesn't know, but gives you directions to the Menswear department. You make your way there and ask your question to an associate standing near the slacks. The associate may not know the answer, but gives you directions to the Bargain Menswear department in the basement behind last year's Christmas decorations. You proceed to that area and ask an associate your question again. This time you're either handed a shirt or given an excuse why one isn't available. 
LDAP uses a similar system of referrals to point clients at the DSA that hosts the naming context containing the requested information. These referrals virtually guarantee the success of any lookup so long as the object exists inside the scope of the information base. 
The key point to remember is that LDAP referrals put the burden of searching on the clients. This contrasts to X.500, where all the messy search work is handed over to the DSAs. LDAP is Wal-Mart to the Nordstroms of X.500. 
RootDSE
When LDAP clients need information from a DSA, they must first bind to the directory service. This authenticates the client and establishes a session for the connection. The client then submits queries for objects and attributes within the directory. This means the client needs to know the security requirements of the DSA along with the structure of the directory service it hosts. 
DSAs “advertise” this information by constructing a special object called RootDSE. The RootDSE object acts like a signpost at a rural intersection. It points the way to various important features in the directory service and gives useful information about the service. LDAP clients use this information to select an authentication mechanism and configure their searches. 
Each DSA constructs its own copy of RootDSE. The information is not replicated between DSAs. RootDSE is like the eye above the pyramid on the back of a dollar bill. It sits apart from the structure but knows all about it. You'll be seeing more about RootDSE later in this book in topics that cover scripting. Querying RootDSE for information about Active Directory rather than hard-coding that information into your scripts is a convenient way to make your scripts portable. 
LDAP Namespace Structure Summary 
Here are the highlights of what you need to remember about the LDAP namespace structure to help you design and administer Active Directory: 
·	An object's full path in the LDAP namespace is called its distinguished name. All DNs must be unique. 
·	The Directory Information Tree, or DIT, is a distributed LDAP database that can be hosted by more than one server. 
·	The DIT is divided into separate units called naming contexts. A domain controller can host more than one naming context.
·	Active Directory uses separate naming contexts to store information about domains in the same DIT.
·	When LDAP clients search for an object, LDAP servers refer the clients to servers that host the naming context containing that object. They do this using shared knowledge about the system topology. 
·	Each DSA creates a RootDSE object that describes the content, controls, and security requirements of the directory service. Clients use this information to select an authentication method and to help formulate their search requests. 
Ref: http://www.zytrax.com/books/ldap/apd/ for indexes

Setup ldap server and client (Ref http://www.learnitguide.net/2016/01/configure-openldap-server-on-rhel7.html )
OpenLDAP Server Configuration on RHEL6
LDAP, or Lightweight Directory Access Protocol, is a protocol for managing related information from a centralized location through the use of a file and directory hierarchy. It functions in a similar way to a relational database in certain ways, and can be used to organize and store any kind of information. LDAP is commonly used for centralized authentication.

Our Lab Setup
 
Description	Server Information	Client Information	   
Operating System	RHEL6 - 64 Bit	RHEL6 - 64 Bit	   
Host Name	host0001.hk.companyDomain	host0002.hk.companyDomain	   
IP Address	abc.de.fgh.jkl	mno.pq.rst.uv	 

Prerequisites:
1. Make sure both server host0001.hk.companyDomain (abc.de.fgh.jkl) and client host0002.hk.companyDomain (mno.pq.rst.uv) are accessible.
2. Make an entry of each host in /etc/hosts for name resolution or configure it in DNS to resolve the IP, if you use server name instead of IP address. Read also How to Configure DNS Server on RHEL7 But we use IP Address for reference.
Server end configuration
Login into the server host0001.hk.companyDomain (abc.de.fgh.jkl) and do the following steps to configure OpenLDAP Server.

1. Install the required LDAP Packages "Openldap"
Install the appropriate LDAP packages "openldap" and "migrationtools" using yum to avoid dependencies issue. if yum is not configured, please refer the link Yum Configuration on Linux
[root@host0001 ~]# yum -y install openldap* migrationtools
2. Create a LDAP root passwd for administration purpose.
[root@host0001 ~]# slappasswd
New password:
Re-enter new password:
{SSHA}dsfkdsfjdsflkdsfkdsfdskfkdfd
Copy the encrypted the passwd from the above output "{SSHA}ffkdsfdsfdfkdsfldsfkdfdkfdsf". Replace with your password and keep it aside.
3. Edit the OpenLDAP Server Configuration
OpenLDAP server Configuration files are located in /etc/openldap/slapd.d/.
Go to cn=config directory under /etc/openldap/slapd.d/ and edit the "olcDatabase={2}hdb.ldif" for changing the configuration.
[root@host0001 ~]# cd /etc/openldap/slapd.d/cn=config
[root@host0001 cn=config]# vi olcDatabase={2}hdb.ldif
Change the variables of "olcSuffix" and "olcRootDN" according to your domain as below.
olcSuffix: dc=hk,dc=companyDomain
olcRootDN: cn=Manager,dc=hk,dc=companyDomain
Add the below three lines additionally in the same configuration file.
olcRootPW: {SSHA}bHSiwuPJEypHS6zHSE2Uy7M69sQjmkPL
olcTLSCertificateFile: /etc/pki/tls/certs/ host0001.pem
olcTLSCertificateKeyFile: /etc/pki/tls/certs/ host0001.pem
Replace the "olcRootPW" value with your copied passwd. Now Save and exit the configuration file.

The suffix line names the domain for which the LDAP server provides information and should be changed to your domain name. The rootdn entry is the Distinguished Name (DN) for a user who is unrestricted by access controls or administrative limit parameters set for operations on the LDAP directory. The rootdn user can be thought of as the root user for the LDAP directory. In the configuration file, change the rootdn line from its default value as above.

4. Provide the Monitor privileges
Open the file /etc/openldap/slapd.d/cn=config/olcDatabase={1}monitor.ldif and go to the line start with olcAccess. Replace the value "dc=my-domain,dc=com" to "dc=hk,dc=companyDomain" as below.
[root@host0001 cn=config]# vi olcDatabase={1}monitor.ldif
olcAccess: {0}to * by dn.base="gidNumber=0+uidNumber=0,cn=peercred,cn=external, cn=auth" read by dn.base="cn=Manager,dc=hk,dc=companyDomain" read by * none
Note: If no olcAccess directives are specified, the default access control policy, to * by * read, allows all users (both authenticated and anonymous) read access.
Note: Access controls defined in the frontend are appended to all other databases' controls.

Verify the configuration
[root@host0001 cn=config]# slaptest -u
56abba86 ldif_read_file: checksum error on "/etc/openldap/slapd.d/cn=config/olcDatabase={1}monitor.ldif"
56abba86 ldif_read_file: checksum error on "/etc/openldap/slapd.d/cn=config/olcDatabase={2}hdb.ldif"
config file testing succeeded
Ignore the Checksum errors as of now.
5. Enable and Start the SLAPD service
[root@host0001 cn=config]# /etc/init.d/slapd start
[root@host0001 cn=config]# chkconfig slapd on
[root@host0001 cn=config]# netstat -lt | grep ldap
tcp        0      0 0.0.0.0:ldap            0.0.0.0:*               LISTEN
tcp6       0      0 [::]:ldap               [::]:*                  LISTEN

6. Configure the LDAP Database
Copy the Sample Database Configuration file, change the file permissions as below.
[root@host0001 cn=config]# cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG
[root@host0001 cn=config]# chown -R ldap:ldap /var/lib/ldap/
Add the following LDAP Schemas
[root@host0001 cn=config]# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine.ldif
[root@host0001 cn=config]# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/nis.ldif
[root@host0001 cn=config]# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson.ldif

7. Create the self-signed certificate
In Step 3, We have specified our certificate locations. But we have not created yet, Lets create the self signed certificate,
[root@host0001 cn=config]# openssl req -new -x509 -nodes -out /etc/pki/tls/certs/ host0001.pem -keyout /etc/pki/tls/certs/ host0001.pem -days 365
Provide your company details to generate the certificate as below.
Country Name (2 letter code) [XX]:IN
State or Province Name (full name) []:Maharashtra
Locality Name (eg, city) [Default City]:Maharashtra
Organization Name (eg, company) [Default Company Ltd]:hk
Organizational Unit Name (eg, section) []:companyDomain
Common Name (eg, your name or your server's hostname) []:host0001.hk.companyDomain
Email Address []:root@ host0001.hk.companyDomain
Verify the created certificates under the location /etc/pki/tls/certs/
[root@host0001 cn=config]# ll /etc/pki/tls/certs/*.pem
-rw-r--r--. 1 root root 1704 Jan  8 14:52 /etc/pki/tls/certs/ host0001.pem
-rw-r--r--. 1 root root 1497 Jan  8 14:52 /etc/pki/tls/certs/ host0001.pem
8. Create base objects in OpenLDAP
To create base objects in OpenLDAP, we need migration tools to be installed. We have already installed the migrationtools in the step 1 itself. So You will see lot of files and scripts under /usr/share/migrationtools/.

We need to change some predefined values in the file "migrate_common.ph" according to our domain name, for that do the following:
[root@host0001 cn=config]# cd /usr/share/migrationtools/
[root@host0001 migrationtools]# vi migrate_common.ph
Go to Line Number 71 and change your domain name
$DEFAULT_MAIL_DOMAIN = "hk.companyDomain";
Go to line number 74 and change your base name
$DEFAULT_BASE = "dc=hk,dc=companyDomain";
Go to line number 90 and change your EXTENDED_SCHEMA from "0" to "1"
$EXTENDED_SCHEMA = 1;
Finally Save and Exit the file.
9. Generate a base.ldif file for your Domain
[root@host0001 migrationtools]# touch /root/base.ldif
Copy the below lines and paste inside the file /root/base.ldif.
dn: dc=hk,dc=companyDomain
objectClass: top
objectClass: dcObject
objectclass: organization
o: hk companyDomain
dc: hk

dn: cn=Manager,dc=hk,dc=companyDomain
objectClass: organizationalRole
cn: Manager
description: Directory Manager

dn: ou=People,dc=hk,dc=companyDomain
objectClass: organizationalUnit
ou: People

dn: ou=Group,dc=hk,dc=companyDomain
objectClass: organizationalUnit
ou: Group
Replace with your domain name instead of hk.companyDomain, Save and exit the file.
10. Create a Local Users
Let’s create some local users and groups, then we will migrate to LDAP. For testing purpose, I create three users as below.
[root@host0001 migrationtools} # useradd ldapuser1
[root@host0001 migrationtools} # useradd ldapuser2
[root@host0001 migrationtools] # echo "ldapuser1" | passwd --stdin ldapuser1
[root@host0001 migrationtools] # echo "ldapuser2" | passwd --stdin ldapuser2
Filter out these user from /etc/passwd to another file:
[root@host0001 migrationtools]# grep ":10[0-9][0-9]" /etc/passwd > /root/passwd
If that doesn’t copy any specific user to /root/passwd then just open /etc/passwd and copy the specific line with the username to the /root/passwd file.
e.g. samir:x:0:0::/home/samir:/bin/bash

Filter out user group from /etc/group to another file:
[root@host0001 migrationtools]# grep ":10[0-9][0-9]" /etc/group > /root/group
Similary if that doesn’t copy any specific group to /root/group then just open /etc/group and copy the specific line with the groupname to the /root/group file.
e.g. root:x:0:samir
Now Convert the Individual Users file to LDAP Data Interchange Format (LDIF)
Generate a ldif file for users
[root@host0001 migrationtools]# ./migrate_passwd.pl /root/passwd /root/users.ldif
Generate a ldif file for groups
[root@host0001 migrationtools]# ./migrate_group.pl /root/group /root/groups.ldif
11. Import Users in to the LDAP Database.
Lets update these ldif file to LDAP Database. 
[root@host0001 migrationtools]# ldapadd -x -W -D "cn=Manager,dc=hk,dc=companyDomain" -f /root/base.ldif
Password: root123
[root@host0001 migrationtools]# ldapadd -x -W -D "cn=Manager,dc=hk,dc=companyDomain" -f /root/users.ldif
Password: root123
[root@host0001 migrationtools]# ldapadd -x -W -D "cn=Manager,dc=hk,dc=companyDomain" -f /root/groups.ldif
Password: root123
Note1:
Please note that while adding new users you may get the following error.
[root@host0001 migrationtools]# ldapadd -x -W -D "cn=Manager,dc=hk,dc=companyDomain" -f /root/users.ldif
Enter LDAP Password:
adding new entry "uid=cassuser,ou=People,dc=hk,dc=companyDomain"
ldap_add: Already exists (68)
[root@host0001 migrationtools]# echo $?
68

Same for groups.ldif. If you check the error code, you’ll find it as 68. This is because, as the user/group in the top of the list in users.ldif or groups.ldif has already been added. When you try to add the same again, it throws error and exits.
To solve the above issue, just move the new user/group appended to the list in users.ldif/groups.ldif, to the top of list. And run the ldapadd command again. This will add the new user/group to the list.
The other way could be, delete all the users/groups by ldapdelete or any relevant command and run the above ldapadd commands.

NOTE: It will ask for a password of "Manager", you have to type the password which you generated in encrypted format or else you can provide the admin user’s direct password(i.e. root123 in this case) i.e. encrypted password generated at the time of or password input at the time of command slappasswd
12. Test the configuration.
To test the configuration, search for the user "ldapuser1" in LDAP as below.
[root@host0001 migrationtools]# ldapsearch -x cn=ldapuser1 -b dc=hk,dc=companyDomain
It prints all the user information:
[root@host0001 migrationtools]# ldapsearch -x -b 'dc=hk,dc=companyDomain '(objectclass=*)'
13. Stop Firewalld to allow the connection.
[root@host0001 migrationtools]# /etc/init.d/iptables stop
[root@host0001 migrationtools]# /etc/init.d/ip6tables stop
LDAP Configuration is done, but we need to share the LDAP Users Home Directories via NFS. So Users who logged in the client servers will also be able to save their data remotely on LDAP Server. If not they will get an error as "Home Directory not found". If you wish to export the Home directory using autofs instead of making an entry in fstab file, please refer the link Mounting the NFS Filesystem using autofs. Here we use simple fstab entry for testing purpose also watch this demo on youtube, how to configure Linux Clients for LDAP Authentication to OpenLDAP Server.
14. NFS Configuration to export the Home Directory.
Edit the file /etc/exports and add an entry as below to export the home directory. 
[root@host0001 ~]# vi /etc/exports
/home *(rw,sync)
Save and Exit the file.
Enable and restart rpcbind and nfs service.
[root@host0001 ~]# yum -y install rpcbind nfs-utils
[root@host0001 ~]# /etc/init.d/rpcbind start
[root@host0001 ~]# /etc/init.d/nfs start
[root@host0001 ~]# chkconfig rpcbind on
[root@host0001 ~]# chkconfig nfs on
Test the NFS Configuration
[root@host0001 u44035615]# showmount -e
Export list for host0001.hk.companyDomain:
/home * 

Test server from server:
Note: ldapuser1 and ldapuser2 are two ldap users

Search all the users:
ldapsearch -x -b 'dc=hk,dc=companyDomain' '(objectclass=*)'
ldapsearch -x cn=Manager -b dc=hk,dc=companyDomain

Search a specific user:
ldapsearch -x cn=ldapuser1 -b dc=hk,dc=companyDomain
ldapsearch -x cn=samir -b dc=hk,dc=companyDomain

Test server from client(logged in as non-root user):
ldapsearch -h hostname -x -b "dc=example,dc=com" 'uid=user'
ldapsearch -x -h hostname  -s sub -b 'dc=hk,dc=companyDomain' "uid=ldapuser1"
e.g.
ldapsearch -h host0001.hk.companyDomain -x -b "dc=hk,dc=companyDomain" 'uid=*'
ldapsearch -h host0001.hk.companyDomain -x -b "dc=hk,dc=companyDomain" 'uid=ldapuser2'
ldapsearch -x -h host0001.hk.companyDomain  -s sub -b 'dc=hk,dc=companyDomain' "uid=ldapuser1"
ldapsearch -h host0001.hk.companyDomain:389 -x -b "ou=people, dc=hk,dc=companyDomain" 'uid=ldapuser2'
ldapsearch -v -h host0001.hk.companyDomain -D uid=ldapuser1,ou=People,dc=hk,dc=companyDomain -w ldapuser1 -b OU=People,DC=hk,DC=companyDomain
ldapsearch -v -h host0001.hk.companyDomain -D uid=samir,ou=People,dc=hk,dc=companyDomain -w "P@ssword01" -b OU=People,DC=hk,DC=companyDomain

Default port set: 389
[root@host0001 u44035615]# netstat -nltp | grep -i slapd
tcp        0      0 0.0.0.0:389                 0.0.0.0:*                   LISTEN      6852/slapd

Sample command and output:
[root@host0001 u44035615]# ldapsearch -h host0001.hk.companyDomain -x -b "ou=People, dc=hk,dc=companyDomain" 'uid=ldapuser2'
# extended LDIF
#
# LDAPv3
# base <ou=People, dc=hk,dc=companyDomain> with scope subtree
# filter: uid=ldapuser2
# requesting: ALL
#

# ldapuser2, People, hk.companyDomain
dn: uid=ldapuser2,ou=People,dc=hk,dc=companyDomain
uid: ldapuser2
cn: ldapuser2
sn: ldapuser2
mail: ldapuser2@hk.companyDomain
objectClass: person
objectClass: organizationalPerson
objectClass: inetOrgPerson
objectClass: posixAccount
objectClass: top
objectClass: shadowAccount
userPassword:: dsafkdsjfdslkfjdslkfdsjflkdsfjdkfdkfsdkfdkfdskfdskfdksfdkfdfkfkdfdksfdkf
 jVUpRcmM3MWJIelFDUzc=
shadowLastChange: 17262
shadowMin: 0
shadowMax: 90
shadowWarning: 7
loginShell: /bin/bash
uidNumber: 10077
gidNumber: 10077
homeDirectory: /home/ldapuser2

# search result
search: 2
result: 0 Success

# numResponses: 2
# numEntries: 1

Client end configuration
Login into the server host0002.hk.companyDomain(mno.pq.rst.uv)

1. Ldap Client Configuration to use LDAP Server
[root@host0002 ~]# yum install -y openldap-clients nss-pam-ldapd
[root@host0002 ~]# authconfig-tui
Steps to follow for LDAP Authentication:
1. Put '*' Mark on "Use LDAP"
2. Put '*' Mark on "Use LDAP Authentication"
3. Select "Next" and Enter.
4. Enter the server field as "ldap:// host0001.hk.companyDomain/"
5. Enter the Base DN Field as "dc=hk,dc=companyDomain"
6. Select "OK" and Enter
 
2. Test the Client Configuration.
Search the ldap user using the below command and check the output. If you get output, then our LDAP Configurations are working properly.
[root@host0002 ~]# getent passwd ldapuser1
ldapuser1:x:1000:1000:ldapuser1:/home/ldapuser1:/bin/bash
3. Mount the LDAP Users Home Directory.
mount -t nfs host0001.hk.companyDomain:/home /home/
Add the below entry to mount the LDAP Users home directory in the file /etc/fstab as below.
host0001.hk.companyDomain:/home   /home   auto  defaults 0 0
If you would like to automount the Home Directories over NFS, please refer the link Automount home directories over NFS using autofs. Configure 
Thats all from client end. Now login using the LDAP User to ensure the authentication.

Difference between export, set, setenv
Every process, even on Windows, has a block of memory known as the environment block, this contains environment variables. When a new process is created, by default, the environment block of the parent process is copied to the child, so environment variables are a simple way of passing text data to a child process.
export makes the variable available to subprocesses. 
That is, if you spawn a new process from your script, the variable k won't be available to that subprocess unless you export it. Note that if you change this variable in the subprocess that change won't be visible in the parent process.
Currently, only simple values can be passed, so items like arrays are not supported (it just exports the first element). Variable attributes, set using define, are also not exported unless the child process is a shell of the same type, i.e. another instance of bash. This also applies to exported functions, although it is possible to sometimes hack this between shells of different types (using eval). 
In Bash (and others) there is a shell setting called allexport which means all variables are environment variables - probably a bad idea to se generally. You can supply a different environemnt block from languages like C using execve, but from the shell you need a program like env, see man env for details.

Example:
root@linux ~# x=5                <= here variable is set without export command
root@linux ~# echo $x
5
root@linux ~# bash               <= subshell creation
root@linux ~# echo $x            <= subshell doesnt know $x variable value
root@linux ~# exit               <= exit from subshell
exit
root@linux ~# echo $x            <= parent shell still knows $x variable
5
root@linux ~# export x=5         <= specify $x variable value using export command
root@linux ~# echo $x            <= parent shell doesn't see any difference from the first declaration
5
root@linux ~# bash               <= create subshell again
root@linux ~# echo $x            <= now the subshell knows $x variable value
5
root@linux ~#

OR
 
down vote	I've created a simple script to show the difference:
$ cat script.sh 
echo $answer
Let's test without export
$ answer=42
$ ./script.sh 

$ . script.sh 
42
The value is known only if using the same process to execute the script (that is, the same bash instance, using source / .)
Now, using export:
$ export answer=42
$ ./script.sh 
42
$ . script.sh 
42
The value is known to the subprocess.
Thus, if you want the value of a variable to be known by subprocesses then you should use export.	 


export and not EXPORT is used by sh and ksh shells. 
set and setenv are the c-shell/tc-shell alternatives for setting a local variable and environment variable respectively. The set command is used for setting local variable, setenv is uesd for setting an environment variable:

   The example below shows the set command usage:
       
 
#set  FILE=”output.txt”
#echo $FILE
output.txt
#tcsh
#echo $FILE

#	 

   The example below shows the setenv command usage:
          
 
#setenv  FILE ”output.txt”
#echo $FILE
output.txt
#tcsh
#echo $FILE
output.txt
#	 

Que:
How to add an admin/root user?	https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux_OpenStack_Platform/2/html/Getting_Started_Guide/ch02s03.html
Ans:
The sudo command offers a mechanism for providing trusted users with administrative access to a system without sharing the password of the root user. When users given access via this mechanism precede an administrative command with sudo they are prompted to enter their own password. Once authenticated, and assuming the command is permitted, the administrative command is executed as if run by the root user. 
Follow this procedure to create a normal user account and give it sudo access. You will then be able to use the sudo command from this user account to execute administrative commands without logging in to the account of the root user. 
⁠
Procedure 2.2. Configuring sudo Access
1.	Log in to the system as the root user. 
2.	Create a normal user account using the useradd command. Replace USERNAME with the user name that you wish to create. 
# useradd USERNAME
3.	Set a password for the new user using the passwd command. 
4.	# passwd USERNAME
5.	Changing password for user USERNAME.
6.	New password: 
7.	Retype new password: 
passwd: all authentication tokens updated successfully.
8.	Run the visudo to edit the /etc/sudoers file. This file defines the policies applied by the sudo command. 
# visudo
9.	Find the lines in the file that grant sudo access to users in the group wheel when enabled. 
10.	## Allows people in group wheel to run all commands
# %wheel        ALL=(ALL)       ALL
11.	Remove the comment character (#) at the start of the second line. This enables the configuration option. 
12.	Save your changes and exit the editor. 
13.	Add the user you created to the wheel group using the usermod command. 
# usermod -aG wheel USERNAME
14.	Test that the updated configuration allows the user you created to run commands using sudo. 
1.	Use the su to switch to the new user account that you created. 
# su USERNAME -
2.	Use the groups to verify that the user is in the wheel group. 
3.	$ groups
USERNAME wheel
4.	Use the sudo command to run the whoami command. As this is the first time you have run a command using sudo from this user account the banner message will be displayed. You will be also be prompted to enter the password for the user account. 
5.	$ sudo whoami
6.	We trust you have received the usual lecture from the local System
7.	Administrator. It usually boils down to these three things:
8.	
9.	    #1) Respect the privacy of others.
10.	    #2) Think before you type.
11.	    #3) With great power comes great responsibility.
12.	
13.	[sudo] password for USERNAME:
root
The last line of the output is the user name returned by the whoami command. If sudo is configured correctly this value will be root. 
You have successfully configured a user with sudo access. You can now log in to this user account and use sudo to run commands as if you were logged in to the account of the root user. 
openssl req -new -x509 -nodes -out /etc/pki/tls/certs/host0001.pem -keyout /etc/pki/tls/certs/host0001.pem -days 365


SAMPLE: http://programtalk.com/vs2/python/13035/RatticWeb/account/models.py/


[root@host0001 u44035615]# vim /etc/exports
[root@host0001 u44035615]# ls
adm.ldif  host0001.hk.companyDomain
[root@host0001 u44035615]# ls
adm.ldif  host0001.hk.companyDomain
[root@host0001 u44035615]# showmount -e
Export list for host0001.hk.companyDomain:
/home *
[root@host0001 u44035615]# vim /etc/exports
[root@host0001 u44035615]# exportfs -i -o ro,sync *:/root
[root@host0001 u44035615]# showmount -e
Export list for host0001.hk.companyDomain:
/home *
/root *

https://www.digitalocean.com/community/tutorials/how-to-configure-openldap-and-perform-administrative-ldap-tasks



Was not able to run ldapsearch command from root. When I was running this from root user it was giving the following error.
[root@host0002 u44035615]# ldapsearch -h host0001.hk.companyDomain -x -b "dc=hk,dc=companyDomain" 'uid=ldapuser2'
ldap_search: No such object
         
This may be because root user is not a part of Active directory. You can check that by doing the following
ldapsearch -h host0001.hk.companyDomain -x -b "dc=hk,dc=companyDomain" 'uid=*' | grep root

So what I added a new user samir to the root user group. I did this on ldap c which is host0001.hk.companyDomain. As the /home directory of the server is exported and mounted on the ldap client(host0002.hk.companyDomain), the username appeared in the /home directory. The command for this is:
sudo useradd -ou 0 -g 0 samir

Though it’s not right; I gave a permission 777 to /home/Samir on the server.

Added the following lines to the /home/Samir/.bashrc on the ldap client.
export ORACLE_BASE=/opt/oraapp/client/
         export ORACLE_HOME=/opt/oraapp/client/12.1.0.2_x64_DBAocl028/
         export LD_LIBRARY_PATH=/opt/oraapp/client/12.1.0.2_x64_DBAocl028/lib/
         PATH=$ORACLE_HOME/bin:$PATH
        . /etc/bashrc 

And then started executing the E2 in the virtual environment.

Add an existing user to the group
[root@host0001 migrationtools]# usermod -a -G root samir
[root@host0001 migrationtools]# vim /etc/group
[root@host0001 migrationtools]# grep samir /etc/group
root:x:0:Samir


If you find syslog(i.e. /var/log/messages) empty and is not logging any system logs then just restart the daemon by the following command
/etc/init.d/rsyslog restart

X86, x64, x86_64, ia64:
Every micro-processor implements an instruction set (also called instruction set architecture or ISA in short). 64-bit ISA or 64-bit processor means that the length of each instruction that the processor executes is 64 bits.

X64, amd64 and x86-64 are names for the same processor type. It's often called amd64 because AMD(Advanced Micro Systems) came up with it initially (1999). All current general-public 64-bit desktops and servers have an amd64 processor. AMD later rebranded it to amd64.
ia64 (itanium) is from intel. Itanium works fast only with 64bits and is still used in industry. Intel now uses x86_64 instructions from AMD due to its popularity in industry.
. There is a processor type called IA-64 or Itanium. It's only found in supercomputers and a few high-end servers. i386 : "i" is Intel , andn 386 comes from Intel's 80386 chips.

Here most difference is between 32bit(i386) and 64bits(x86_64 and ia64). You cannot run app for 64bit on 32bit cpu but in reverse usually yes.
x86_64 (AMD64) cpu is most common instruction set as comes to 64bit cpu on desktop computer. It is from AMD which was few years earlier with their cpu which worked fine with x86(32-bits) instructions also.

In hardware, x86_64 is a type of processor that can run both 32bit and 64bit applications just fine where ia64 runs 32bit applications SLOWER than any other CPU, as it is meant for 64bit only applications.

x86-64 (also known as x64, x86_64 and AMD64[note 1]) is the 64-bit version of the x86 instruction set.
x86-64 == x64== x86_64 ==amd64 is the 64-bit version of the x86 instruction set.
A Graphics Processing Unit (GPU) is a special purpose processor, optimized for calculations commonly (and repeatedly) required for Computer Graphics, particularly SIMD operations.
A Central Processing Unit (CPU) is a general purpose processor - it can in principle do any computation, but not necessarily in an optimal fashion for any given computation. One can do graphics processing on a CPU - but it likely will not produce the result anywhere nearly as fast as a properly programmed GPU.

Find all 0 byte files:
find $dir -size 0
Note that not all implementations of find will produce output by default, so you may need to do: find $dir-size 0 –print
For listing with ls
find $dir -size 0 -ls
If you would like to delete those files
find $dir -size 0 -type f –delete

Adding a user and giving it sudo privileges:
useradd newuser1
echo "newuser1password" | passwd --stdin newuser1
sudo usermod -a -G root newuser1
Add this user to /etc/sudoers to act like a sudo user


http://michal.karzynski.pl/blog/2014/05/18/setting-up-an-asynchronous-task-queue-for-django-using-celery-redis/
Setting up an asynchronous task queue for Django using Celery and Redis
Celery is a powerful, production-ready asynchronous job queue, which allows you to run time-consuming Python functions in the background. A Celery powered application can respond to user requests quickly, while long-running tasks are passed onto the queue. In this article we will demonstrate how to add Celery to a Django application using Redis.
Celery uses a broker to pass messages between your application and Celery worker processes. In this article we will set up Redis as the message broker. You should note that persistence is not the main goal of this data store, so your queue could be erased in the event of a power failure or other crash. Keep this in mind and don’t use the job queue to store application state. If you need your queue to be have persistence, use another message broker such as RabbitMQ.

Synchronous execution means the execution happens in a single series. A->B->C->D. If you are calling those routines, A will run, then finish, then B will start, then finish, then C will start, etc.
With Asynchronous execution, you begin a routine, and let it run in the background while you start your next, then at some point, say "wait for this to finish". It's more like:
Start A->B->C->D->Wait for A to finish
The advantage is that you can execute B, C, and or D while A is still running (in the background, on a separate thread), so you can take better advantage of your resources and have fewer "hangs" or "waits".

In simpler terms:
SYNCHRONOUS
You are in a queue to get a movie ticket. You cannot get one until everybody in front of you gets one, and the same applies to the people queued behind you.
ASYNCHRONOUS
You are in a restaurant with many other people. You order your food. Other people can also order their food, they don't have to wait for your food to be cooked and served to you before they can order. In the kitchen restaurant workers are continuously cooking, serving, and taking orders. People will get their food served as soon as it is cooked.
This document describes the current stable version of Celery (4.1). For development docs, go here. 
http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html
First Steps with Celery¶
Celery is a task queue with batteries included. It’s easy to use. It’s designed around best practices so that your product can scale and integrate with other languages, and it comes with the tools and support you need to run such a system in production.
Learn about;
•	Choosing and installing a message transport (broker).
•	Installing Celery and creating your first task.
•	Starting the worker and calling tasks.
•	Keeping track of tasks as they transition through different states, and inspecting return values.
Celery may seem daunting at first - but don’t worry - this tutorial will get you started in no time. It’s deliberately kept simple, so as to not confuse you with advanced features. After you have finished this tutorial, it’s a good idea to browse the rest of the documentation. For example the Next Steps tutorial will showcase Celery’s capabilities.
•	Choosing a Broker
o	RabbitMQ
o	Redis
o	Other brokers
•	Installing Celery
•	Application
•	Running the Celery worker server
•	Calling the task
•	Keeping Results
•	Configuration
•	Where to go from here
•	Troubleshooting
o	Worker doesn’t start: Permission Error
o	Result backend doesn’t work or tasks are always in PENDING state
Choosing a Broker¶
Celery requires a solution to send and receive messages; usually this comes in the form of a separate service called a message broker.
There are several choices available, including:
RabbitMQ¶
RabbitMQ is feature-complete, stable, durable and easy to install. It’s an excellent choice for a production environment. Detailed information about using RabbitMQ with Celery:
Using RabbitMQ
If you’re using Ubuntu or Debian install RabbitMQ by executing this command:
$ sudo apt-get install rabbitmq-server
When the command completes, the broker will already be running in the background, ready to move messages for you: Starting rabbitmq-server: SUCCESS.
Don’t worry if you’re not running Ubuntu or Debian, you can go to this website to find similarly simple installation instructions for other platforms, including Microsoft Windows:
http://www.rabbitmq.com/download.html
Redis¶
Redis is also feature-complete, but is more susceptible to data loss in the event of abrupt termination or power failures. Detailed information about using Redis:
Using Redis
Other brokers¶
In addition to the above, there are other experimental transport implementations to choose from, including Amazon SQS.
See Broker Overview for a full list.
Installing Celery¶
Celery is on the Python Package Index (PyPI), so it can be installed with standard Python tools like pip or easy_install:
$ pip install celery
Application
The first thing you need is a Celery instance. As this instance is used as the entry-point for everything you want to do in Celery, like creating tasks and managing workers, it must be possible for other modules to import it.
In this tutorial we keep everything contained in a single module, but for larger projects you want to create a dedicated module.
Let’s create the file tasks.py:
from celery import Celery

app = Celery('tasks', broker='pyamqp://guest@localhost//')

@app.task
def add(x, y):
    return x + y
The first argument to Celery is the name of the current module. This is only needed so that names can be automatically generated when the tasks are defined in the __main__ module.
The second argument is the broker keyword argument, specifying the URL of the message broker you want to use. Here using RabbitMQ (also the default option).
See Choosing a Broker above for more choices – for RabbitMQ you can use amqp://localhost, or for Redis you can use redis://localhost.
You defined a single task, called add, returning the sum of two numbers.
Running the Celery worker server¶
You can now run the worker by executing our program with the worker argument:
$ celery -A tasks worker --loglevel=info
Note
See the Troubleshooting section if the worker doesn’t start.
In production you’ll want to run the worker in the background as a daemon. To do this you need to use the tools provided by your platform, or something like supervisord (see Daemonization for more information).
For a complete listing of the command-line options available, do:
$  celery worker --help
There are also several other commands available, and help is also available:
$ celery help
Calling the task
To call our task you can use the delay() method.
This is a handy shortcut to the apply_async() method that gives greater control of the task execution (see Calling Tasks):
>>> from tasks import add
>>> add.delay(4, 4)
The task has now been processed by the worker you started earlier. You can verify this by looking at the worker’s console output.
Calling a task returns an AsyncResult instance. This can be used to check the state of the task, wait for the task to finish, or get its return value (or if the task failed, to get the exception and traceback).
Results are not enabled by default. In order to do remote procedure calls or keep track of task results in a database, you will need to configure Celery to use a result backend. This is described in the next section.
Keeping Results
If you want to keep track of the tasks’ states, Celery needs to store or send the states somewhere. There are several built-in result backends to choose from: SQLAlchemy/Django ORM, Memcached, Redis, RPC (RabbitMQ/AMQP), and – or you can define your own.
For this example we use the rpc result backend, that sends states back as transient messages. The backend is specified via the backend argument to Celery, (or via the result_backend setting if you choose to use a configuration module):
app = Celery('tasks', backend='rpc://', broker='pyamqp://')
Or if you want to use Redis as the result backend, but still use RabbitMQ as the message broker (a popular combination):
app = Celery('tasks', backend='redis://localhost', broker='pyamqp://')
To read more about result backends please see Result Backends.
Now with the result backend configured, let’s call the task again. This time you’ll hold on to the AsyncResult instance returned when you call a task:
>>> result = add.delay(4, 4)
The ready() method returns whether the task has finished processing or not:
>>> result.ready()
False
You can wait for the result to complete, but this is rarely used since it turns the asynchronous call into a synchronous one:
>>> result.get(timeout=1)
8
In case the task raised an exception, get() will re-raise the exception, but you can override this by specifying the propagate argument:
>>> result.get(propagate=False)
If the task raised an exception, you can also gain access to the original traceback:
>>> result.traceback
?
See celery.result for the complete result object reference.
Configuration¶
Celery, like a consumer appliance, doesn’t need much configuration to operate. It has an input and an output. The input must be connected to a broker, and the output can be optionally connected to a result backend. However, if you look closely at the back, there’s a lid revealing loads of sliders, dials, and buttons: this is the configuration.
The default configuration should be good enough for most use cases, but there are many options that can be configured to make Celery work exactly as needed. Reading about the options available is a good idea to familiarize yourself with what can be configured. You can read about the options in the Configuration and defaults reference.
The configuration can be set on the app directly or by using a dedicated configuration module. As an example you can configure the default serializer used for serializing task payloads by changing the task_serializer setting:
app.conf.task_serializer = 'json'
If you’re configuring many settings at once you can use update:
app.conf.update(
    task_serializer='json',
    accept_content=['json'],  # Ignore other content
    result_serializer='json',
    timezone='Europe/Oslo',
    enable_utc=True,
)
For larger projects, a dedicated configuration module is recommended. Hard coding periodic task intervals and task routing options is discouraged. It is much better to keep these in a centralized location. This is especially true for libraries, as it enables users to control how their tasks behave. A centralized configuration will also allow your SysAdmin to make simple changes in the event of system trouble.
You can tell your Celery instance to use a configuration module by calling the app.config_from_object() method:
app.config_from_object('celeryconfig')
This module is often called “celeryconfig”, but you can use any module name.
In the above case, a module named celeryconfig.py must be available to load from the current directory or on the Python path. It could look something like this:
celeryconfig.py:
broker_url = 'pyamqp://'
result_backend = 'rpc://'

task_serializer = 'json'
result_serializer = 'json'
accept_content = ['json']
timezone = 'Europe/Oslo'
enable_utc = True
To verify that your configuration file works properly and doesn’t contain any syntax errors, you can try to import it:
$ python -m celeryconfig
For a complete reference of configuration options, see Configuration and defaults.
To demonstrate the power of configuration files, this is how you’d route a misbehaving task to a dedicated queue:
celeryconfig.py:
task_routes = {
    'tasks.add': 'low-priority',
}
Or instead of routing it you could rate limit the task instead, so that only 10 tasks of this type can be processed in a minute (10/m):
celeryconfig.py:
task_annotations = {
    'tasks.add': {'rate_limit': '10/m'}
}
If you’re using RabbitMQ or Redis as the broker then you can also direct the workers to set a new rate limit for the task at runtime:
$ celery -A tasks control rate_limit tasks.add 10/m
worker@example.com: OK
    new rate limit set successfully
See Routing Tasks to read more about task routing, and the task_annotations setting for more about annotations, or Monitoring and Management Guide for more about remote control commands and how to monitor what your workers are doing.
Where to go from here¶
If you want to learn more you should continue to the Next Steps tutorial, and after that you can read the User Guide.
Troubleshooting¶
There’s also a troubleshooting section in the Frequently Asked Questions.
Worker doesn’t start: Permission Error¶
•	If you’re using Debian, Ubuntu or other Debian-based distributions:
Debian recently renamed the /dev/shm special file to /run/shm.
A simple workaround is to create a symbolic link:
# ln -s /run/shm /dev/shm
•	Others:
If you provide any of the --pidfile, --logfile or --statedb arguments, then you must make sure that they point to a file or directory that’s writable and readable by the user starting the worker.
Result backend doesn’t work or tasks are always in PENDING state¶
All tasks are PENDING by default, so the state would’ve been better named “unknown”. Celery doesn’t update the state when a task is sent, and any task with no history is assumed to be pending (you know the task id, after all).
1.	Make sure that the task doesn’t have ignore_result enabled.
Enabling this option will force the worker to skip updating states.
2.	Make sure the task_ignore_result setting isn’t enabled.
3.	Make sure that you don’t have any old workers still running.
It’s easy to start multiple workers by accident, so make sure that the previous worker is properly shut down before you start a new one.
An old worker that isn’t configured with the expected result backend may be running and is hijacking the tasks.
The --pidfile argument can be set to an absolute path to make sure this doesn’t happen.
4.	Make sure the client is configured with the right backend.
If, for some reason, the client is configured to use a different backend than the worker, you won’t be able to receive the result. Make sure the backend is configured correctly:
>>> result = task.delay()
>>> print(result.backend)


Prerequisites
In this article we will add Celery to a Django application running in a Python virtualenv. I will assume that the virtual environment is located in the directory /webapps/hello_django/ and that the application is up an running. You can follow steps in my previous article to set up Django in virtualenv running on Nginx and Gunicorn.
This article was tested on a server running Debian 7.
Install Redis
$ sudo aptitude install redis-server
$ redis-server --version
Redis server version 2.4.14 (00000000:0)
Check if Redis is up and accepting connections:
$ redis-cli ping
PONG
Installing Celery in your aplication’s virtualenv
 (hello_django)hello@django:~$ pip install celery[redis]
Downloading/unpacking celery[redis]
(...)
Successfully installed celery pytz billiard kombu redis anyjson amqp
Cleaning up...
Setting up Celery support in your Django application
In order to use Celery as part of your Django application you’ll need to create a few files and tweak some settings. Let’s start by adding Celery-related configuration variables to settings.py
/webapps/hello_django/hello/hello/settings.py 
1
2
3
4
5
	# CELERY SETTINGS
BROKER_URL = 'redis://localhost:6379/0'
CELERY_ACCEPT_CONTENT = ['json']
CELERY_TASK_SERIALIZER = 'json'
CELERY_RESULT_SERIALIZER = 'json'

Now we’ll create a file named celery.py, which will instantiate Celery, creating a so called Celery application. You can find more information about available Celery application settings in the documentation.
/webapps/hello_django/hello/hello/celery.py 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
	from __future__ import absolute_import

import os
from celery import Celery
from django.conf import settings

# set the default Django settings module for the 'celery' program.
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'hello.settings')

app = Celery('hello_django')

# Using a string here means the worker will not have to
# pickle the object when using Windows.
app.config_from_object('django.conf:settings')
app.autodiscover_tasks(lambda: settings.INSTALLED_APPS)
In order to instantiate the Celery app every time our Django application is started, we can add the following lines to the __init__.py file in our Django proj.proj module. This will make sure that celery task use this app.
/webapps/hello_django/hello/hello/__init__.py 
1
2
	from __future__ import absolute_import
from .celery import app as celery_app

Create your first tasks
We will now add an app called testapp to our Django project and add some tasks to this app. Let’s start by creating the app:
(hello_django)hello@django:~/hello$ python manage.py startapp testapp
Make sure that the app is added to INSTALLED_APPS in settings.py
/webapps/hello_django/hello/hello/settings.py 
1
2
3
4
	INSTALLED_APPS = (
    # (...)
    'testapp',
)

Create a file called tasks.py in your apps’s directory and add the code of your first Celery task to the file.
/webapps/hello_django/hello/testapp/tasks.py 
1
2
3
4
5
6
7
	from __future__ import absolute_import

from celery import shared_task

@shared_task
def test(param):
    return 'The test task executed with argument "%s" ' % param

Find more information about writing task functions in the docs.
If you created all files as outlined above, you should see the following directory structure:
/webapps/hello_django/hello
├── hello
│   ├── celery.py       # The Celery app file
│   ├── __init__.py     # The project module file we modified
│   ├── settings.py     # Settings go here, obviously :)
│   ├── urls.py
│   └── wsgi.py
├── manage.py
└── testapp
    ├── __init__.py
    ├── models.py
    ├── tasks.py        # File containing tasks for this app
    ├── tests.py
    └── views.py
You can find a complete sample Django project on Celery’s GitHub.
Testing the setup
In production we will want Celery workers to be daemonized, but let’s just quickly start the workers to check that everything is configured correctly. Use the celery command located in your virtualenv’s bin directory to start the workers. Make sure that the module path hello.celery:app is available on your PYTHONPATH.
It’s important to understand how Celery names tasks which it discovers and how these names are related to Python module paths. If you run into NotRegistered or ImportError exceptions make sure that your apps and tasks are imported in a consistent manner and your PYTHONPATH is set correctly.
$ export PYTHONPATH=/webapps/hello_django/hello:$PYTHONPATH
$ /webapps/hello_django/bin/celery --app=hello.celery:app worker --loglevel=INFO

 -------------- celery@django v3.1.11 (Cipater)
---- **** -----
--- * ***  * -- Linux-3.2.0-4-amd64-x86_64-with-debian-7.5
-- * - **** ---
- ** ---------- [config]
- ** ---------- .> app:         hello_django:0x15ae410
- ** ---------- .> transport:   redis://localhost:6379/0
- ** ---------- .> results:     disabled
- *** --- * --- .> concurrency: 2 (prefork)
-- ******* ----
--- ***** ----- [queues]
 -------------- .> celery           exchange=celery(direct) key=celery

[tasks]
  . testapp.tasks.test

[2014-05-20 13:53:59,740: INFO/MainProcess] Connected to redis://localhost:6379/0
[2014-05-20 13:53:59,748: INFO/MainProcess] mingle: searching for neighbors
[2014-05-20 13:54:00,756: INFO/MainProcess] mingle: all alone
[2014-05-20 13:54:00,769: WARNING/MainProcess] celery@django ready.
If everything worked, you should see a splash screen similar to the above and the [tasks] section should list tasks discovered in all the apps of your project.
[tasks]
  . testapp.tasks.test
Submitting a task to the queue for execution
In another terminal, activate the virtualenv and start a task from your project’s shell.
$ sudo su - hello
hello@django:~$ source bin/activate
(hello_django)hello@django:~$ cd hello/
(hello_django)hello@django:~/hello$ python manage.py shell
Python 2.7.3 (default, Mar 13 2014, 11:03:55)
[GCC 4.7.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
(InteractiveConsole)
>>> from testapp.tasks import test
>>> test.delay('This is just a test!')
<AsyncResult: 79e35cf7-0a3d-4786-b746-2d3dd45a5c16>
You should see messages appear in the terminal where Celery workers are started:
[2014-05-18 11:43:24,801: INFO/MainProcess] Received task: testapp.tasks.test[79e35cf7-0a3d-4786-b746-2d3dd45a5c16]
[2014-05-18 11:43:24,804: INFO/MainProcess] Task testapp.tasks.test[79e35cf7-0a3d-4786-b746-2d3dd45a5c16] succeeded in 0.00183034200018s: u'The test task executed with argument "This is just a test!" '
You can find more information about calling Celery tasks in the docs.
Running Celery workers as daemons
In production we can use supervisord to start Celery workers and make sure they are restarted in case of a system reboot or crash. Installation of Supervisor is simple:
$ sudo aptitude install supervisor
When Supervisor is installed you can give it programs to start and watch by creating configuration files in the /etc/supervisor/conf.d directory. For our hello-celery worker we’ll create a file named /etc/supervisor/conf.d/hello-celery.conf with this content:
/etc/supervisor/conf.d/hello-celery.conf 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
	[program:hello-celery]
command=/webapps/hello_django/bin/celery --app=hello.celery:app worker --loglevel=INFO
directory=/webapps/hello_django/hello
user=hello
numprocs=1
stdout_logfile=/webapps/hello_django/logs/celery-worker.log
stderr_logfile=/webapps/hello_django/logs/celery-worker.log
autostart=true
autorestart=true
startsecs=10

; Need to wait for currently executing tasks to finish at shutdown.
; Increase this if you have very long running tasks.
stopwaitsecs = 600

; When resorting to send SIGKILL to the program to terminate it
; send SIGKILL to its whole process group instead,
; taking care of its children as well.
killasgroup=true

; if rabbitmq is supervised, set its priority higher
; so it starts first
priority=998

This configuration is based on a sample config provided by the makers of Celery. You can set many other options.
Create a file to store your application’s log messages:
hello@django:~$ mkdir -p /webapps/hello_django/logs/
hello@django:~$ touch /webapps/hello_django/logs/celery-worker.log
After you save the configuration file for your program you can ask supervisor to reread configuration files and update (which will start your the newly registered app).
$ sudo supervisorctl reread
hello-celery: available
$ sudo supervisorctl update
hello-celery: added process group
You can now monitor output of Celery workers by following the celery-worker.log file:
$ tail -f /webapps/hello_django/logs/celery-worker.log
You can also check the status of Celery or start, stop or restart it using supervisor.
$ sudo supervisorctl status hello                       
hello                            RUNNING    pid 18020, uptime 0:00:50
$ sudo supervisorctl stop hello  
hello: stopped
$ sudo supervisorctl start hello                        
hello: started
$ sudo supervisorctl restart hello 
hello: stopped
hello: started
Celery workers should now be automatically started after a system reboot and automatically restarted if they ever crashed for some reason.
Inspecting worker tasks
You can check that Celery is running by issuing the celery status command:
$ export PYTHONPATH=/webapps/hello_django/hello:$PYTHONPATH
$ /webapps/hello_django/bin/celery --app=hello.celery:app status
celery@django: OK

1 node online.
You can also inspect the queue using a friendly curses monitor:
$ export PYTHONPATH=/webapps/hello_django/hello:$PYTHONPATH
$ /webapps/hello_django/bin/celery --app=hello.celery:app control enable_events
$ /webapps/hello_django/bin/celery --app=hello.celery:app events
 
Celery Worker monitor
I hope that’s enough to get you started. You should probably read the Celery User Guide now. Happy coding!

https://realpython.com/blog/python/caching-in-django-with-redis/
Getting Started
We have created an example application to introduce you to the concept of caching. Our application uses:
•	Django (v1.9.8)
•	Django Debug Toolbar (v1.4)
•	django-redis (v4.4.3)
•	Redis (v3.2.0)
Install the App
Visit the admin page in the browser to confirm that the data has been properly loaded.
1
2
3
4
5
	(django-redis)$ python manage.py makemigrations cookbook
(django-redis)$ python manage.py migrate
(django-redis)$ python manage.py createsuperuser
(django-redis)$ python manage.py loaddata cookbook/fixtures/cookbook.json
(django-redis)$ python manage.py runserver

Once you have the Django app running, move onto the Redis installation.
Install Redis
Once installed, Run the Redis server from a new terminal window.
$ redis-server
	
Next, start up the Redis command-line interface (CLI) in a different terminal window and test that it connects to the Redis server. We will be using the Redis CLI to inspect the keys that we add to the cache.

$ redis-cli ping
PONG

Redis provides an API with various commands that a developer can use to act on the data store. Django uses django-redis to execute commands in Redis.

Looking at our example app in a text editor, we can see the Redis configuration in the settings.py file. We define a default cache with the CACHES setting, using a built-in django-redis cache as our backend. Redis runs on port 6379 by default, and we point to that location in our setting. One last thing to mention is that django-redis appends key names with a prefix and a version to help distinguish similar keys. In this case, we have defined the prefix to be “example”.
1
2
3
4
5
6
7
8
9
10
	CACHES = {
    "default": {
        "BACKEND": "django_redis.cache.RedisCache",
        "LOCATION": "redis://127.0.0.1:6379/1",
        "OPTIONS": {
            "CLIENT_CLASS": "django_redis.client.DefaultClient"
        },
        "KEY_PREFIX": "example"
    }
}

NOTE: Although we have configured the cache backend, none of the view functions have implemented caching.
App Performance
As we mentioned at the beginning of this tutorial, everything that the server does to process a request slows the application load time. The processing overhead of running business logic and rendering templates can be significant. Network latency affects the time it takes to query a database. These factors come into play every time a client sends an HTTP request to the server. When users are initiating many requests per second, the effects on performance become noticeable as the server works to process them all.
When we implement caching, we let the server process a request once and then we store it in our cache. As requests for the same URL are received by our application, the server pulls the results from the cache instead of processing them anew each time. Typically, we set a time to live on the cached results, so that the data can be periodically refreshed, which is an important step to implement in order to avoid serving stale data.
You should consider caching the result of a request when the following cases are true:
•	rendering the page involves a lot of database queries and/or business logic,
•	the page is visited frequently by your users,
•	the data is the same for every user,
•	and the data does not change often.
Start by Measuring Performance
Begin by testing the speed of each page in your application by benchmarking how quickly your application returns a response after receiving a request.
To achieve this, we’ll be blasting each page with a burst of requests using loadtest, an HTTP load generator, and then paying close attention to the request rate. Visit the link above to install. Once installed, test the results against the /cookbook/ URL path:
1
	$ loadtest -n 100 -k  http://localhost:8000/cookbook/

Notice that we are processing about 16 requests per second:
1
	Requests per second: 16

When we look at what the code is doing, we can make decisions on how to make changes to improve the performance. The application makes 3 network calls to a database with each request to /cookbook/, and it takes time for each call to open a connection and execute a query. Visit the /cookbook/ URL in your browser and expand the Django Debug Toolbar tab to confirm this behavior. Find the menu labeled “SQL” and read the number of queries:
 

cookbook/services.py
1
2
3
4
5
6
7
	from cookbook.models import Recipe


def get_recipes():
    # Queries 3 tables: cookbook_recipe, cookbook_ingredient,
    # and cookbook_food.
    return list(Recipe.objects.prefetch_related('ingredient_set__food'))

cookbook/views.py
1
2
3
4
5
6
7
8
	from django.shortcuts import render
from cookbook.services import get_recipes


def recipes_view(request):
    return render(request, 'cookbook/recipes.html', {
        'recipes': get_recipes()
    })

The application also renders a template with some potentially expensive logic.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
	<html>
<head>
  <title>Recipes</title>
</head>
<body>
{% for recipe in recipes %}
  <h1>{{ recipe.name }}</h1>
    <p>{{ recipe.desc }}</p>
  <h2>Ingredients</h2>
  <ul>
    {% for ingredient in recipe.ingredient_set.all %}
    <li>{{ ingredient.desc }}</li>
    {% endfor %}
  </ul>
  <h2>Instructions</h2>
    <p>{{ recipe.instructions }}</p>
{% endfor %}
</body>
</html>

Implement Caching
Imagine the total number of network calls that our application will make as users start to visit our site. If 1,000 users hit the API that retrieves cookbook recipes, then our application will query the database 3,000 times and a new template will be rendered with each request. That number only grows as our application scales. Luckily, this view is a great candidate for caching.
The recipes in a cookbook rarely change, if ever. Also, since viewing cookbooks is the central theme of the app, the API retrieving the recipes is guaranteed to be called frequently.
In the example below, we modify the view function to use caching. When the function runs, it checks if the view key is in the cache. If the key exists, then the app retrieves the data from the cache and returns it. If not, Django queries the database and then stashes the result in the cache with the view key. The first time this function is run, Django will query the database and render the template, and then will also make a network call to Redis to store the data in the cache. Each subsequent call to the function will completely bypass the database and business logic and will query the Redis cache.
example/settings.py
1
2
	# Cache time to live is 15 minutes.
CACHE_TTL = 60 * 15

cookbook/views.py
1
2
3
4
5
6
7
8
9
10
11
12
13
14
	from django.conf import settings
from django.core.cache.backends.base import DEFAULT_TIMEOUT
from django.shortcuts import render
from django.views.decorators.cache import cache_page
from cookbook.services import get_recipes

CACHE_TTL = getattr(settings, 'CACHE_TTL', DEFAULT_TIMEOUT)


@cache_page(CACHE_TTL)
def recipes_view(request):
    return render(request, 'cookbook/recipes.html', {
        'recipes': get_recipes()
    })

Notice that we have added the @cache_page() decorator to the view function, along with a time to live. Visit the /cookbook/ URL again and examine the Django Debug Toolbar. We see that 3 database queries are made and 3 calls are made to the cache in order to check for the key and then to save it. Django saves two keys (1 key for the header and 1 key for the rendered page content). Reload the page and observe how the page activity changes. The second time around, 0 calls are made to the database and 2 calls are made to the cache. Our page is now being served from the cache!
When we re-run our performance tests, we see that our application is loading faster.
1
	$ loadtest -n 100 -k  http://localhost:8000/cookbook/

Caching improved the total load, and we are now resolving 21 requests per second, which is 5 more than our baseline:
1
	Requests per second: 21

Inspecting Redis with the CLI
At this point we can use the Redis CLI to look at what gets stored on the Redis server. In the Redis command-line, enter the keys * command, which returns all keys matching any pattern. You should see a key called “example:1:views.decorators.cache.cache_page”. Remember, “example” is our key prefix, “1” is the version, and “views.decorators.cache.cache_page” is the name that Django gives the key. Copy the key name and enter it with the get command. You should see the rendered HTML string.
1
2
3
4
5
	$ redis-cli -n 1
127.0.0.1:6379[1]> keys *
1) "example:1:views.decorators.cache.cache_header"
2) "example:1:views.decorators.cache.cache_page"
127.0.0.1:6379[1]> get "example:1:views.decorators.cache.cache_page"

NOTE: Run the flushall command on the Redis CLI to clear all of the keys from the data store. Then, you can run through the steps in this tutorial again without having to wait for the cache to expire.
Wrap-up
Processing HTTP requests is costly, and that cost adds up as your application grows in popularity. In some instances, you can greatly reduce the amount of processing your server does by implementing caching. This tutorial touched on the basics of caching in Django with Redis, but it only skimmed the surface of a complex topic. Implementing caching in a robust application has many pitfalls and gotchas. Controlling what gets cached and for how long is tough. Cache invalidation is one of the hard things in Computer Science. Ensuring that private data can only be accessed by its intended users is a security issue and must be handled very carefully when caching. Play around with the source code in the example application and as you continue to develop with Django, remember to always keep performance in mind.

https://realpython.com/blog/python/flask-by-example-implementing-a-redis-task-queue/

Flask by Example - Implementing a Redis Task Queue 
This part of the tutorial details how to implement a Redis task queue to handle text processing.
________________________________________
Remember: Here’s what we’re building – A Flask app that calculates word-frequency pairs based on the text from a given URL.

1.	Part One: Set up a local development environment and then deploy both a staging and a production environment on Heroku.
2.	Part Two: Set up a PostgreSQL database along with SQLAlchemy and Alembic to handle migrations.
3.	Part Three: Add in the back-end logic to scrap and then process the word counts from a webpage using the requests, BeautifulSoup, and Natural Language Toolkit (NLTK) libraries.
4.	Part Four: Implement a Redis task queue to handle the text processing. (current)
5.	Part Five: Set up Angular on the front-end to continuously poll the back-end to see if the request is done processing.
6.	Part Six: Push to the staging server on Heroku – setting up Redis and detailing how to run two processes (web and worker) on a single Dyno.
7.	Part Seven: Update the front-end to make it more user-friendly.
8.	Part Eight: Create a custom Angular Directive to display a frequency distribution chart using JavaScript and D3.
Need the code? Grab it from the repo.
Install Requirements
Tools used:
•	Redis (3.0.7)
•	Python Redis (2.10.5)
•	RQ (0.5.6) – a simple library for creating a task queue
Start by downloading and installing Redis from either the official site or via Homebrew (brew install redis). Once installed, start the Redis server:
1
	$ redis-server

Next install Python Redis and RQ in a new terminal window:
1
2
3
	$ cd flask-by-example
$ pip install redis==2.10.5 rq==0.5.6
$ pip freeze > requirements.txt

Set up the Worker
Let’s start by creating a worker process to listen for queued tasks. Create a new file worker.py, and add this code:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
	import os

import redis
from rq import Worker, Queue, Connection

listen = ['default']

redis_url = os.getenv('REDISTOGO_URL', 'redis://localhost:6379')

conn = redis.from_url(redis_url)

if __name__ == '__main__':
    with Connection(conn):
        worker = Worker(list(map(Queue, listen)))
        worker.work()

Here, we listened for a queue called default and established a connection to the Redis server on localhost:6379.
Fire this up in another terminal window:
1
2
3
4
5
	$ cd flask-by-example
$ python worker.py
17:01:29 RQ worker started, version 0.5.6
17:01:29
17:01:29 *** Listening on default...

Now we need to update our app.py to send jobs to the queue…
Update app.py
Add the following imports to app.py:
1
2
3
	from rq import Queue
from rq.job import Job
from worker import conn

Then update the configuration section:
1
2
3
4
5
6
7
8
	app = Flask(__name__)
app.config.from_object(os.environ['APP_SETTINGS'])
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = True
db = SQLAlchemy(app)

q = Queue(connection=conn)

from models import *

q = Queue(connection=conn) set up a Redis connection and initialized a queue based on that connection.
Move the text processing functionality out of our index route and into a new function called count_and_save_words(). This function accepts one argument, a URL, which we will pass to it when we call it from our index route.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
	def count_and_save_words(url):

    errors = []

    try:
        r = requests.get(url)
    except:
        errors.append(
            "Unable to get URL. Please make sure it's valid and try again."
        )
        return {"error": errors}

    # text processing
    raw = BeautifulSoup(r.text).get_text()
    nltk.data.path.append('./nltk_data/')  # set the path
    tokens = nltk.word_tokenize(raw)
    text = nltk.Text(tokens)

    # remove punctuation, count raw words
    nonPunct = re.compile('.*[A-Za-z].*')
    raw_words = [w for w in text if nonPunct.match(w)]
    raw_word_count = Counter(raw_words)

    # stop words
    no_stop_words = [w for w in raw_words if w.lower() not in stops]
    no_stop_words_count = Counter(no_stop_words)

    # save the results
    try:
        result = Result(
            url=url,
            result_all=raw_word_count,
            result_no_stop_words=no_stop_words_count
        )
        db.session.add(result)
        db.session.commit()
        return result.id
    except:
        errors.append("Unable to add item to database.")
        return {"error": errors}


@app.route('/', methods=['GET', 'POST'])
def index():
    results = {}
    if request.method == "POST":
        # get url that the person has entered
        url = request.form['url']
        if 'http://' not in url[:7]:
            url = 'http://' + url
        job = q.enqueue_call(
            func=count_and_save_words, args=(url,), result_ttl=5000
        )
        print(job.get_id())

    return render_template('index.html', results=results)

Take note of the following code:
1
2
3
4
	job = q.enqueue_call(
    func=count_and_save_words, args=(url,), result_ttl=5000
)
print(job.get_id())

Here we used the queue that we initialized earlier and called the enqueue_call() function. This added a new job to the queue and that job ran the count_and_save_words() function with the URL as the argument. The result_ttl=5000 line argument tells RQ how long to hold on to the result of the job for – 5,000 seconds, in this case. Then we outputted the job id to the terminal. This id is needed to see if the job is done processing.
Let’s setup a new route for that…
Get Results
1
2
3
4
5
6
7
8
9
	@app.route("/results/<job_key>", methods=['GET'])
def get_results(job_key):

    job = Job.fetch(job_key, connection=conn)

    if job.is_finished:
        return str(job.result), 200
    else:
        return "Nay!", 202

Let’s test this out.
Fire up the server, navigate to http://localhost:5000/, use the URL http://realpython.com, and grab the job id from the terminal. Then use that id in the ‘/results/’ endpoint – i.e., http://localhost:5000/results/ef600206-3503-4b87-a436-ddd9438f2197.
As long as less than 5,000 seconds have elapsed before you check the status, then you should see an id number, which is generated when we add the results to the database:
1
2
3
4
5
6
7
8
9
10
11
	# save the results
try:
    from models import Result
    result = Result(
        url=url,
        result_all=raw_word_count,
        result_no_stop_words=no_stop_words_count
    )
    db.session.add(result)
    db.session.commit()
    return result.id

Now, let’s refactor the route slightly to return the actual results from the database in JSON:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
	@app.route("/results/<job_key>", methods=['GET'])
def get_results(job_key):

    job = Job.fetch(job_key, connection=conn)

    if job.is_finished:
        result = Result.query.filter_by(id=job.result).first()
        results = sorted(
            result.result_no_stop_words.items(),
            key=operator.itemgetter(1),
            reverse=True
        )[:10]
        return jsonify(results)
    else:
        return "Nay!", 202

Make sure to add the import:
1
	from flask import jsonify

Test this out again. If all went well, you should see something similar to in your browser:
1
2
3
4
5
6
7
8
9
10
11
12
	{
  Course: 5,
  Python: 19,
  Real: 11,
  course: 4,
  courses: 7,
  development: 7,
  product: 4,
  sample: 4,
  videos: 5,
  web: 12
}

http://www.bogotobogo.com/python/python_redis_with_python.php
To create a connection to Redis using redis-py:
$ python
Python 3.4.3 (default, Oct 14 2015, 20:28:29) 
[GCC 4.8.4] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import redis
>>> r = redis.Redis(host='localhost', port=6379, db=0)
The port=6379 and db=0 are default values.



Reading and Writing Data
Now that we are connected to Redis, we can start reading and writing data.
The following code snippet writes the value bar to the Redis key foo, reads it back, and prints it: 
>>> r.set('foo','bar')
True
>>> r.get('foo')
b'bar
The set key to hold the string value. If key already holds a value, it is overwritten, regardless of its type. Any previous time to live associated with the key is discarded on successful SET operation.



incr/decr
The incr/decr increments/decrements the number stored at key by one.
If the key does not exist, it is set to 0 before performing the operation.
>>> r.set('count',1)
True
>>> r.incr('count')
2
>>> r.incr('count')
3
>>> r.decr('count')
2
>>> r.get('count')
b'2'
Error case:
>>> r.set('count',123456789012345678901234567890)
True
>>> r.incr('count')
redis.exceptions.ResponseError: value is not an integer or out of range



rpush, llen, and lindex
The rpush inserts all the specified values at the tail of the list stored at key.
The llen returns the length of the list stored at key.
The lindex returns the element at index index in the list stored at key. The index is zero-based, so 0 means the first element, 1 the second element and so on. 
>>> r.rpush('hispanic', 'uno')
1
>>> r.rpush('hispanic', 'dos')
2
>>> r.rpush('hispanic', 'tres')
3
>>> r.rpush('hispanic', 'cuatro')
4
>>> r.llen('hispanic')
4
>>> r.lindex('hispanic', 3)
b'cuatro'


You mean other than the fact that Redis is a key-value database and RabbitMQ is a messaging system? Seriously though... I don't know what RestMQ is, but if it is something that sits on top of Redis, it is something that is trying to be something that Redis isn't. RabbitMQ on the other hand is arguably the best message-oriented-middleware on the planet. 
While I am also using RabbitMQ quite happily, I'm currently exploring a Redis broker, since the AMQP protocol is likely overkill for my logging use case.
We are defining an architecture to collect log information by Logstash shippers which are installed in various machines and index the data in one elasticsearch server centrally and use Kibana as the graphical layer. We need a reliable messaging system in between Logstash shippers and elasticsearch to grantee the delivery. What factors should be considered when selecting Redis over RabbitMQ as a data broker/messaging system in between Logstash shippers and the elasticsearch or vice versa?

Redis is created as a key value data store despite having some basic message broker capabilities.
RabbitMQ is created as a message broker. It has lots of message broker capabilities naturally.
I have been doing some research on this topic. If performance is important and persistence is not, RabbitMQ is a perfect choice. Redis is a technology developed with a different intent. 
Following are my list of pros for using RabbitMQ over Redis:
•	RabbitMQ uses AMQP protocol which can be configured to use SSL, additional layer of security. By using SSL certificates it can encrypt the data that you are sending to the broker and it means that no one will sniff your data and have access to your vital organizational data.
•	RabbitMQ takes approximately 75% of the time Redis takes in accepting messages. 
•	RabbitMQ supports priorities for messages, which can be used by workers to consume high priority messages first. 
•	There is no chance of loosing the message if any worker crashes after consuming the message, which is not the case with Redis. 
•	RabbitMQ has a good routing system to direct messages to different queues.
•	Regarding scaling, RabbitMQ has a built in cluster implementation that you can use in addition to a load balancer in order to implement a redundant broker environment.
A few cons for using RabbitMQ:
•	RabbitMQ might be a little hard to maintain, hard to debug crashes. 
•	node-name or node-ip fluctuations can cause data loss, but if managed well, durable messages can solve the problem.
While I am also using RabbitMQ quite happily, I'm currently exploring a Redis broker, since the AMQP protocol is likely overkill for my logging use case.

https://www.rabbitmq.com/features.html
What can RabbitMQ do for you?
Messaging enables software applications to connect and scale. Applications can connect to each other, as components of a larger application, or to user devices and data. Messaging is asynchronous, decoupling applications by separating sending and receiving data. 
You may be thinking of data delivery, non-blocking operations or push notifications. Or you want to use publish / subscribe, asynchronous processing, or work queues. All these are patterns, and they form part of messaging. 
RabbitMQ is a messaging broker - an intermediary for messaging. It gives your applications a common platform to send and receive messages, and your messages a safe place to live until received. 
Feature Highlights
Reliability
RabbitMQ offers a variety of features to let you trade off performance with reliability, including persistence, delivery acknowledgements, publisher confirms, and high availability. 
Flexible Routing
Messages are routed through exchanges before arriving at queues. RabbitMQ features several built-in exchange types for typical routing logic. For more complex routing you can bind exchanges together or even write your own exchange type as a plugin. 
Clustering
Several RabbitMQ servers on a local network can be clustered together, forming a single logical broker. 
Federation
For servers that need to be more loosely and unreliably connected than clustering allows, RabbitMQ offers a federation model. 
Highly Available Queues
Queues can be mirrored across several machines in a cluster, ensuring that even in the event of hardware failure your messages are safe. 
Multi-protocol
RabbitMQ supports messaging over a variety of messaging protocols. 
Many Clients
There are RabbitMQ clients for almost any language you can think of. 
Management UI
RabbitMQ ships with an easy-to use management UI that allows you to monitor and control every aspect of your message broker. 
Tracing
If your messaging system is misbehaving, RabbitMQ offers tracing support to let you find out what's going on. 
Plugin System
RabbitMQ ships with a variety of plugins extending it in different ways, and you can also write your own. 
And Also...
Commercial Support
Commercial support, training and consulting are available from Pivotal. 
Large Community
There's a large community around RabbitMQ, producing all sorts of clients, plugins, guides, etc. Join our mailing list to get involved! 


https://spring.io/understanding/AMQP
l Understanding AMQP
AMQP (Advanced Message Queueing Protocol) is an openly published wire specification for asynchronous messaging. Every byte of transmitted data is specified. This characteristic allows libraries to be written in many languages, and to run on multiple operating systems and CPU architectures, which makes for a truly interoperable, cross-platform messaging standard.
l Advantages of AMQP over JMS
AMQP is often compared to JMS (Java Message Service), the most common messaging system in the Java community. A limitation of JMS is that the APIs are specified, but the message format is not. Unlike AMQP, JMS has no requirement for how messages are formed and transmitted. Essentially, every JMS broker can implement the messages in a different format. They just have to use the same API.
Thus Pivotal has released a JMS on Rabbit project, a library that implements the JMS APIs but uses RabbitMQ, an AMQP broker, to transmit the messages.
AMQP publishes its specifications in a downloadable XML format. This availability makes it easy for library maintainers to generate APIs driven by the specs while also automating construction of algorithms to marshal and demarshal messages.
These advantages and the openness of the spec have inspired the creation of multiple brokers that support AMQP, including:
•	RabbitMQ 
•	ActiveMQ 
•	Qpid 
•	Solace
l AMQP and JMS terminology
•	JMS has queues and topics. A message sent on a JMS queue is consumed by no more than one client. A message sent on a JMS topic may be consumed by multiple consumers. AMQP only has queues. While AMQP queues are only consumed by a single receiver, AMQP producers don't publish directly to queues. A message is published to an exchange, which through its bindings may get sent to one queue or multiple queues, effectively emulating JMS queues and topics. 
•	JMS and AMQP have an equivalent message header, providing the means to sort and route messages. 
•	JMS and AMQP both have brokers responsible for receiving, routing, and ultimately dispensing messages to consumers. 
•	AMQP has exchanges, routes, and queues. Messages are first published to exchanges. Routes define on which queue(s) to pipe the message. Consumers subscribing to that queue then receive a copy of the message. If more than one consumer subscribes to the same queue, the messages are dispensed in a round-robin fashion.

https://blog.docker.com/2016/03/containers-are-not-vms/

I spend a good portion of my time at Docker talking to community members with varying degrees of familiarity with Docker and I sense a common theme: people’s natural response when first working with Docker is to try and frame it in terms of virtual machines. I can’t count the number of times I have heard Docker containers described as “lightweight VMs”
.
I get it because I did the exact same thing when I first started working with Docker. It’s easy to connect those dots as both technologies share some characteristics. Both are designed to provide an isolated environment in which to run an application. Additionally, in both cases that environment is represented as a binary artifact that can be moved between hosts. There may be other similarities, but to me these are the two biggies.

The key is that the underlying architecture is fundamentally different between the two. The analogy I use (because if you know me, you know I love analogies) is comparing houses (VMs) to apartment buildings (containers).

Houses (the VMs) are fully self-contained and offer protection from unwanted guests. They also each possess their own infrastructure – plumbing, heating, electrical, etc. Furthermore, in the vast majority of cases houses are all going to have at a minimum a bedroom, living area, bathroom, and kitchen. I’ve yet to ever find a “studio house” – even if I buy the smallest house I may end up buying more than I need because that’s just how houses are built.  (for the pedantic out there, yes I’m ignoring the new trend in micro houses because they break my analogy).

Apartments (the containers) also offer protection from unwanted guests, but they are built around shared infrastructure. The apartment building (Docker Host) shares plumbing, heating, electrical, etc. Additionally apartments are offered in all kinds of different sizes – studio to multi-bedroom penthouse. You’re only renting exactly what you need. Finally, just like houses, apartments have front doors that lock to keep out unwanted guests.
With containers, you share the underlying resources of the Docker host and you build an image that is exactly what you need to run your application. You start with the basics and you add what you need. VMs are built in the opposite direction. You are going to start with a full operating system and, depending on your application, might be strip out the things you don’t want.
I’m sure many of you are saying “yeah, we get that. They’re different”. But even as we say this, we still try and adapt our current thoughts and processes around VMs and apply them to containers.
•	“How do I backup a container?”
•	“What’s my patch management strategy for my running containers?”
•	“Where does the application server run?”
To me the light bulb moment came when I realized that Docker is not a virtualization technology, it’s an application delivery technology. In a VM-centered world, the unit of abstraction is a monolithic VM that stores not only application code, but often its stateful data. A VM takes everything that used to sit on a physical server and just packs it into a single binary so it can be moved around.  But it is still the same thing.  With containers the abstraction is the application; or more accurately a service that helps to make up the application.
With containers, typically many services (each represented as a single container) comprise an application. Applications are now able to be deconstructed into much smaller components which fundamentally changes the way they are managed in production.
So, how do you backup your container, you don’t. Your data doesn’t live in the container, it lives in a named volume that is shared between 1-N containers that you define. You backup the data volume, and forget about the container. Optimally your containers are completely stateless and immutable.
Certainly patches will still be part of your world, but they aren’t applied to running containers. In reality if you patched a running container, and then spun up new ones based on an unpatched image, you’re gonna have a bad time. Ideally you would update your Docker image, stop your running containers, and fire up new ones. Because a container can be spun up in a fraction off a second, it’s just much cheaper to go this route.
Your application server translates into a service run inside of a container. Certainly there may be cases where your microservices-based application will need to connect to a non-containerized service, but for the most part standalone servers where you execute your code give way to one or more containers that provide the same functionality with much less overhead (and offer up much better horizontal scaling).
“But, VMs have traditionally been about lift and shift. What do I do with my existing apps?”
I often have people ask me how to run huge monolithic apps in a container. There are many valid strategies for migrating to a microservices architecture that start with moving an existing monolithic application from a VM into a container but that should be thought of as the first step on a journey, not an end goal.
As you consider how your organization can leverage Docker, try and move away from a VM-focused mindset and realize that Docker is way more than just “a lightweight VM.” It’s an application-centric way to  deliver high-performing, scalable applications on the infrastructure of your choosing.





RQ (Redis Queue) is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis and it is designed to have a low barrier to entry. It can be integrated in your web stack easily. RQ requires Redis >= 2.6.0.

What is a session cache? 
Session Cache is a caching service that stores and persists HTTP session objects to a remote data grid. The data grid is a general use grid that stores strings and objects. Session Cache remotely leverages the caching capabilities of the data grid and lets you store session data.

LRU is a cache eviction algorithm called least recently used cache. Look at this resource. LFU is a cache eviction algorithm called least frequently used cache. It requires three data structures. One is a hash table which is used to cache the key/values so that given a key we can retrieve the cache entry at O(1).


The logic for implementing any cache is the following:
Check if object exists in cache by fetching it.
If it doesn't exist, calculate the object (or generate it) and put it in the cache.
Return the object.
Django provides a simple dictionary-like API for caches. Once you have correctly configured the cache, you can use the simple cache api:
from django.core.cache import cache

def get(request):
   value = cache.get('somekey')
   if not value:
      # The value in the cache for the key 'somekey' has expired
      # or doesn't exist, so we generate the value
      value = 42
      cache.set('somekey', value)


What is celery used for Python? 
Celery is an asynchronous task queue/job queue based on distributed message passing. It is focused on real-time operation, but supports scheduling as well. The execution units, called tasks, are executed concurrently on a single or more worker servers using multiprocessing, Eventlet, or gevent.

What is the difference between cookies and session? 
The main difference between a session and a cookie is that session data is stored on the server, whereas cookies store data in the visitor's browser. Sessions are more secure than cookies as it is stored in server.Cookie can be turned off from browser. Jun 14, 2011

What is the difference between a cookie and a cache? 
Difference between Cache and Cookies. Cookie is a very small piece of information that is stored on the client's machine by the web site and is sent back to the server each time a page is requested. ... Cache is a temporary storage of web page resources stored on client's machine for quicker loading of the web pages.

What is Elasticache Amazon? 
Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud. ... ElastiCache for Redis is fully managed, scalable, and secure - making it an ideal candidate to power high-performance use cases such as Web, Mobile Apps, Gaming, Ad-Tech, and IoT.

Why do we need memcached? 
Memcached is not typically used on the same machine as the application server, the reason for this is because it is designed to be used via TCP (it would be accessible via sockets if it were designed to work on the same server) and it was designed as a pooling server.May 26, 2012

Is Redis distributed? 
A single instance is therefore limited by the maximum memory of your server. Now, you can also share the data to several Redis instances, running on multiple servers. ... Regarding question 2, a single Redis instance is not a distributed system. It is a remote centralized store.

What is meant by LRU Least Recently Used? 
Least recently used. (Operating systems) (LRU) A rule used in a paging system which selects a page to be paged out if it has been used (read or written) less recently than any other page. The same rule may also be used in a cache to select which cache entry to flush.
What is the LRU cache? 
Writing an LRU Cache. ... It's a cache that, when low on memory, evicts least recently used items. LRU is an eviction policy that makes a lot of sense for the typical kind of cache we all deal with on a daily basis

Let's consider a constant stream of cache requests with a cache capacity of 3, see below: 
A, B, C, A, A, A, A, A, A, A, A, A, A, A, B, C, D
If we just consider a Least Recently Used (LRU) cache with a HashMap + doubly linked list implementation with O(1) eviction time and O(1) load time, we would have the following elements cached while processing the caching requests as mentioned above. 
[A]
[A, B]
[A, B, C]
[B, C, A] <- a stream of As keeps A at the head of the list.
[C, A, B]
[A, B, C]
[B, C, D] <- here, we evict A, we can do better! 
When you look at this example, you can easily see that we can do better - given the higher expected chance of requesting an A in the future, we should not evict it even if it was least recently used. 
A - 12
B - 2
C - 2
D - 1
Least Frequently Used (LFU) cache takes advantage of this information by keeping track of how many times the cache request has been used in its eviction algorithm.
It requires three data structures. One is a hash table which is used to cache the key/values so that given a key we can retrieve the cache entry at O(1). Second one is a double linked list for each frequency of access. The max frequency is capped at the cache size to avoid creating more and more frequency list entries. If we have a cache of max size 4 then we will end up with 4 different frequencies. Each frequency will have a double linked list to keep track of the cache entries belonging to that particular frequency. The third data structure would be to somehow link these frequencies lists. It can be either an array or another linked list so that on accessing a cache entry it can be easily promoted to the next frequency list in time O(1).
The main difference is that in LRU we only check on which page is recently that used old in time than other pages i.e checking only based on recent used pages. BUT in LFU we check the old page as well as the frequency of that page and if frequency of the page is lager than the old page we cant remove it and if we all old pages are having same frequency then take last i.e FIFO method for that. and remove page....

https://realpython.com/blog/python/asynchronous-tasks-with-django-and-celery/

Q. What is Amazon EC2 service?

Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable (scalable) computing capacity in the cloud. You can use Amazon EC2 to launch as many virtual servers you need. In Amazon EC2 you can configure security and networking as well as manage storage.Amazon EC2 service also helps in obtaining and configuring capacity using minimal friction.

Q. What is the difference between Scalability and Elasticity?

Scalability is the ability of a system to increase its hardware resources to handle the increase in demand. It can be done by increasing the hardware specifications or increasing the processing nodes.

Elasticity is the ability of a system to handle increase in the workload by adding additional hardware resources when the demand increases(same as scaling) but also rolling back the scaled resources, when the resources are no longer needed. This is particularly helpful in Cloud environments, where a pay per use model is followed.


Q. What are the features of the Amazon EC2 service?

As the Amazon EC2 service is a cloud service so it has all the cloud features. Amazon EC2 provides the following features:

Virtual computing environment (known as instances)

Pre-configured templates for your instances (known as Amazon Machine Images – AMIs)

Amazon Machine Images (AMIs) is a complete package that you need for your server (including the operating system and additional software)

Amazon EC2 provides various configurations of CPU, memory, storage and networking capacity for your instances (known as instance type)

Secure login information for your instances using key pairs (AWS stores the public key and you can store the private key in a secure place)

Storage volumes of temporary data is deleted when you stop or terminate your instance (known as instance store volumes)

Amazon EC2 provides persistent storage volumes (using Amazon Elastic Block Store – EBS)

A firewall that enables you to specify the protocols, ports, and source IP ranges that can reach your instances using security groups

Static IP addresses for dynamic cloud computing (known as Elastic IP address)

Amazon EC2 provides metadata (known as tags)

Amazon EC2 provides virtual networks that are logically isolated from the rest of the AWS cloud, and that you can optionally connect to your own network (known as virtual private clouds – VPCs)


Q. What is Amazon Machine Image and what is the relation between Instance and AMI?

Amazon Web Services provides several ways to access Amazon EC2, like web-based interface, AWS Command Line Interface (CLI) and Amazon Tools for Windows Powershell. First, you need to sign up for an AWS account and you can access Amazon EC2.
 Amazon EC2 provides a Query API. These requests are HTTP or HTTPS requests that use the HTTP verbs GET or POST and a Query parameter named Action.
Interested in mastering AWS Training? Enroll now for FREE demo on AWS Training.


Q. What is Amazon Machine Image (AMI)?

An Amazon Machine Image (AMI) is a template that contains a software configuration (for example, an operating system, an application server, and applications). From an AMI, we launch an instance, which is a copy of the AMI running as a virtual server in the cloud. We can even launch multiple instances of an AMI.


Q. What is the relation between Instance and AMI?

We can launch different types of instances from a single AMI. An instance type essentially determines the hardware of the host computer used for your instance. Each instance type offers different compute and memory capabilities.

After we launch an instance, it looks like a traditional host, and we can interact with it as we would do with any computer. We have complete control of our instances; we can use sudo to run commands that require root privileges.


Q. Explain storage for Amazon EC2 instance.

Amazon EC2 provides many data storage options for your instances. Each option has a unique combination of performance and durability. These storages can be used independently or in combination to suit your requirements.

There are mainly four types of storages provided by AWS.

Amazon EBS: Its durable, block-level storage volumes  can be attached in running Amazon EC2 instance. The Amazon EBS volume persists independently from the running life of an Amazon EC2 instance. After an EBS volume is attached to an instance, you can use it like any other physical hard drive. Amazon EBS encryption feature supports encryption feature.
Amazon EC2 Instance Store: Storage disk that is attached to the host computer is referred to as instance store. The instance storage provides temporary block-level storage for Amazon EC2 instances. The data on an instance store volume persists only during the life of the associated Amazon EC2 instance; if you stop or terminate an instance, any data on instance store volumes is lost.
Amazon S3: Amazon S3 provides access to reliable and inexpensive data storage infrastructure. It is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within Amazon EC2 or anywhere on the web.
Adding Storage: Every time you launch an instance from an AMI, a root storage device is created for that instance. The root storage device contains all the information necessary to boot the instance. You can specify storage volumes in addition to the root device volume when you create an AMI or launch an instance using block device mapping.


Q. What are the Security Best Practices for Amazon EC2?

There are several best practices for secure Amazon EC2. Following are few of them.


Use AWS Identity and Access Management (AM) to control access to your AWS resources.

Restrict access by only allowing trusted hosts or networks to access ports on your instance.

Review the rules in your security groups regularly, and ensure that you apply the principle of least

Privilege — only open up permissions that you require.

Disable password-based logins for instances launched from your AMI. Passwords can be found or cracked, and are a security risk.


Q. Explain Stopping, Starting, and Terminating an Amazon EC2 instance?

Stopping and Starting an instance: When an instance is stopped, the instance performs a normal shutdown and then transitions to a stopped state. All of its Amazon EBS volumes remain attached, and you can start the instance again at a later time. You are not charged for additional instance hours while the instance is in a stopped state.
Terminating an instance: When an instance is terminated, the instance performs a normal shutdown, then the attached Amazon EBS volumes are deleted unless the volume’s deleteOnTermination attribute is set to false. The instance itself is also deleted, and you can’t start the instance again at a later time.


Q. Explain Elastic Block Storage?  What type of performance can you expect?  How do you back it up?  How do you improve performance?

EBS is a virtualized SAN or storage area network.  That means it is RAID storage to start with, so it’s redundant and fault tolerant.  If disks die in that RAID you don’t lose data.  Great!  It is also virtualized, so you can provision and allocate storage, and attach it to your server with various API calls.  No calling the storage expert and asking him or her to run specialized commands from the hardware vendor.

Performance on EBS can exhibit variability.  That is, it can go above the SLA performance level, then drop below it.  The SLA provides you with an average disk I/O rate you can expect.  This can frustrate some folks, especially performance experts who expect reliable and consistent disk throughout on a server.  Traditional physically hosted servers behave that way.  Virtual AWS instances do not.

Backup EBS volumes by using the snapshot facility via API call or via a GUI interface like elasticfox.

Improve performance by using Linux software raid and striping across four volumes.


Q. What is S3?  What is it used for?  Should encryption be used?

S3 stands for Simple Storage Service.  You can think of it like FTP storage, where you can move files to and from there, but not mount it like a filesystem.  AWS automatically puts your snapshots there, as well as AMIs there.  Encryption should be considered for sensitive data, as S3 is a proprietary technology developed by Amazon themselves, and as yet unproven vis-a-vis a security standpoint.


Q. What is an AMI?  How do I build one?

AMI stands for Amazon Machine Image.  It is effectively a snapshot of the root filesystem.  Commodity hardware, servers have a bios that points the master boot record of the first block on a disk.  A disk image, though can sit anywhere physically on a disk, so Linux can boot from an arbitrary location on the EBS storage network.

Build a new AMI by first spinning up and instance from a trusted AMI.  Then adding packages and components as required.  Be wary of putting sensitive data onto an AMI.  For instance, your access credentials should be added to an instance after spinup  with a database, mount an outside volume that holds your MySQL data after spinup as well.


Q. Can I vertically scale an Amazon instance?  How?

Yes.  This is an incredible feature of AWS and cloud virtualization.  Spin up a new larger instance than the one you are currently running.  Pause that instance and detach the root ebs volume from this server and discard.  Then stop your live instance, detach its root volume.  Note down the unique device ID and attach that root volume to your new server.   And then start it again.  Voila, you have scaled vertically in-place!!


Q. What is auto-scaling?  How does it work?

Auto-scaling is a feature of AWS which allows you to configure and automatically provision and spin up new instances without the need for your intervention.  You do this by setting thresholds and metrics to monitor.  When those thresholds are crossed, a new instance of your choosing will be spun up, configured, and rolled into the load balancer pool.  Voila, you’ve scaled horizontally without any operator intervention!


Q. What automation tools can I use to spin up servers?

The most obvious way is to roll-your-own scripts, and use the AWS API tools.  Such scripts could be written in bash, Perl or another language or your choice. The next option is to use a configuration management and provisioning tools like puppet or better it’s successor Opscode Chef.  You might also look towards a tool like Scalr.  Lastly, you can go with a managed solution such as Rightscale.


Q. What is configuration management?  Why would I want to use it with cloud provisioning of resources?

Configuration management has been around for a long time in web operations and systems administration.  Yet the cultural popularity of it has been limited.  Most systems administrators configure machines as software was developed before version control – that is manually making changes on servers.  Each server can then and usually is slightly different.  Troubleshooting though, is straightforward as you login to the box and operate on it directly.  Configuration management brings a large automation tool in the picture, managing servers like strings of a puppet.  This forces standardization, best practices, and reproducibility as all configs are versioned and managed.  It also introduces a new way of working which is the biggest hurdle to its adoption.

Enter the cloud, then configuration management becomes even more critical.  That’s because virtual servers such as amazons EC2 instances are much less reliable than physical ones.  You absolutely need a mechanism to rebuild them as-is at any moment.  This pushes best practices like automation, reproducibility and disaster recovery into center stage.


Q. Explain how you would simulate perimeter security using the Amazon Web Services model?

Traditional perimeter security that we’re already familiar with using firewalls and so forth is not supported in the Amazon EC2 world.  AWS supports security groups.  One can create a security group for a jump box with ssh access – only port 22 open.  From there a webserver group and database group are created.  The webserver group allows 80 and 443 from the world, but port 22 *only* from the jump box group.  Further the database group allows port 3306 from the webserver group and port 22 from the jump box group.  Add any machines to the webserver group and they can all hit the database.  No one from the world can, and no one can directly ssh to any of your boxes.


Q. How to use Amazon SQS?

Amazon SQS (Simple Queue Service) is a message passing mechanism that is used for communication between different connectors that are connected with each other. It also acts as a communicator between various components of Amazon. It keeps all the different functional components together. This functionality helps different components to be loosely coupled, and provide an architecture that is more failure resilient system.


